{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pL7OrdnLVoy"
   },
   "source": [
    "# Laboratoire 6: Réseaux de neurones récurrents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sftgX3Cu0v-R"
   },
   "source": [
    "## Références\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "https://distill.pub/2016/augmented-rnns/\n",
    "\n",
    "https://github.com/mila-udem/ecolehiver2018/blob/master/Tutoriaux/Hiver18_0308_tutorial_rnn_lstm_solution.ipynb\n",
    "\n",
    "### jean.philippe.reid@rd.mila.quebec\n",
    "\n",
    "### Mars 2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADKCAYAAACrHYtRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztnc+PG9eW37/tX7RGtkX5x7yZDPVUzsNk8IIkamUWGWIGEDWIdyRE7bKKStsEHLX+AlO7ZKUWiGSr6n0AUyB3HowpZAKuBurOZjIYTEQ98eXl5fnZ7Gf7yfTI7izuqe5i8d6q+6u6i+T5AILdbNapSzZ56tS553zP1tHRERiGYZj15ZWzXgDDMAxTLOzoGYZh1hx29AzDMGsOO3qGYZg1hx09wzDMmsOOnmEYZs1hR88wDLPmsKNnGIZZc9jRMwzDrDmvnfUCmGUqvXEDQCP1cDTv1CenvhiiOZgGAKrDVm3fs81g2KqNfNlkGGaZLZZAOHsqvfE2gC6AG5qHXJ936qPCFpSiOZgmPySPh61aw4PNLoCPfdtlGGYZTt2cMZXeuArgCfSdPAB8VumNo2JWlMs1T3baBdllGCYFp2480hxMGwD6AC6kfrUHIEqnKCq9cRvAJ5anu1XpjW8BuD3v1CNLGzbsebITAbif+PmRJ7sMw6TgiN4DzcG0S+mNz7Ds5AHgFoDPkimQSm88g72TT/KQcvpFcgDgLoAHANrNwbTqwWabbD6GuHg0PNhkGEYC5+gdaQ6mEYQj12Xv08nzHQBfelzGIYBg3qnPPNoEADQH010A3WGrNqOf9wFg2KptO9qdDVu1auLnfVebDMPI4YjegeZg2oaZk8fXL17cgl8nD4i7CN820RxMZwAmsZMHjh182BxMQ0ub22knH9tNbfoyDOMJdvRuRKYH/N3hbwpYhn8oPdMftmq76d9RieVuczC1icD79E/GXUubDMNkwI7eDVk+PpMvvv2uiHUAAHxV4lDp42TYqoWq51BEbnQ+SgNtq+zSRSVqDqaqCwHDMBawo7eE0jZlw9eaPgYQ6pyPnHculOq5k0wDqWyCN2YZxivs6O3ZMT3g86++LmIdSYzvMNLQpujWsFXLjaqHrdoEwIRy+Vk2qwAaw1ZtS8fmsFWrNgfTGadxGMYPXEdPVHrjAMJ538l42iMAfapbN27w+dW331qt7bRoDqY70Ivkjxm2arvNwTRqDqYjWWcrRfxhevNVgwDACAA7e4ZxhCN6HDv5p8h28oDoXn1Y6Y1DiNpyI377/ffGazstqNnrvo2WDaVjVA75DixSSmQzpIsPwzAObLSjr/TG25XeeB/CyZvw8BeHv7lier7Xt0r9dnd1UisqZOmW5mA6oTTQyNLmPoAgLzXEMEw2m566GcEyrz377jv8vuEx77z+On75ojzpG6puqQKYwc9Gbh+iaiaAeG+dHfSwVduh2vs+RDqnq7N/wDDMCRvp6ClVsw+HzcvpNy/w0w/Mjnn/XAV//5uvbE/pFeroNRFS0yW+07kB4JlHu/FaPwFgfefBMJvIRjp6mKdqpPzDr7/AT957V/v5b5075+O0WZgIg6U3RxseUiRB+gHK/buyZJdhGH02ztFTNO+F//3VN0aO/hQwSWnsIhHR+0iHUKPVZ4mHuj6GipDdh652GGZT2ThRM1KNdK43T3L1vYt4/+23tJ7735///D9++/0P/8Xn+WPmnbpVSoPq3HezOmE17RwBuBlfNGSaNhY2Q1qbD8VMhtlISl0GUhBenTwAPPn1l/ju5Uudp948/A//5r9CSPP6xjofTqWMbZdSRqq2uZe6Mwg9dBA/hEVzGsMwJ2xiRF/oC/4ouHQRoukormbZl6UvfK6jdv5c/6cfvH+XOlWtSUsSGx4rlRmOq2VsJIhZuphh/LBRET3NZi2UYas2I3GuEYSzb9Bgkm6qpf+Br3P+9IP32wCeUlmjNcNWbQdi3UZQDj1U/Fr1eJ7NXfAmLMN4YaMi+kpv3MXiQGrvfBRc0n3q7WGrFrlE9n/4ztsI3l1KXe+55NrpYrQ9bNUizefPIHLo3ZznHQG4rrM5a7oGhmGyYUfvGQNHH3Pv08nzGYAuNPcP/vCdt3Gp+g5efUV9Q+bS5Uo19o1hqxZoPG9HN9XTHExHANpZz6fnVDllwzD+2KjUTUn5eN6px2mKXP2cf37xAoJ3q5lOHjiupLFlBzldrbRxm+m0JbQhyjqz2IY/uWWGYbB5EX0Di3Xe3rGI6GOWUi4eRutdtNlYTZ3/ZrrGnvYCdiinb2ozBPBQdsfhoxyTYZhlNiqin3fqo7NeQwYLs2ddN1YJ18j4LuRTpPo2Th4AKO/+IP366A5kZGOTYZhsNiqiB4otrzz32qv4s9o/cTFxHNVT/tto8LiEZ3l59jzIAUf0rwqRemnYyBmn7E4gUkTbZNdGs15mN4Co9JnwZi7DCDYqoi+aH51709XErUSkG2Q8T5fLrgYo9XMDQkzsIUSFjZOTJ0Ky+TGEZr2vpqh9svnQcZ+CYdaGtdO6odK8+As+kTQRfQhPomZJaufP4Q/96N48RbnVGT9uDqY+7KQd+y4Mh42nob99snKpD54/yzDr4egpDSCNXhNO6eqwVdufd+qTSm98gBM5XS/89IP3fZoDRL7aeFxhiiKkFvbyauZ1oL9ZUqhMa8h4FsNWbT91EXK2yTDrwMqnbqiDUidF8SSh5RICOPS1htr5QuSHfaRHfNgoyma6GqiIKVKTAmwyzMqx0puxlhuWD+KKER9Klg7llEri0kPH8krnjVhawwiJenmHYd9LdpPDxE2bryT2AghdoWrisX0AM9nQcobZJFY2oid9FZuqlOQA8AYcVB+Dt8/bHnoadF0NkFPfTjpfuki6VtxEWC793IGZnn6aPpbX1YZ6aDnDbAwrG9E7Rrv3knlm00YqHQkCF5LNRJav89BDxL0NUUYpzXOTxs2OaQkjvZ7bsuNoGlXVdAgKNWHtq6qBaK3OJaEMs6qsZETvoZkoTP5AjVTXAezlHHfwwbnKf9aRIPBIrixCikM4VprEtfMqJ0/swHDqE9ndU10cSPAsSql85tnchui0zXLifThW9DDMKrOSET1pnLsOtr5nWz3iQZogE5U8APL3E5yUK1Pn0oqAdTXj43RN3p1G3CGrK2pmcP4AYm+hq2OXYdaJVS2vdHXywIrVV8cOkiLYECL3PIHoFej6Oo+F3kzYHEyPstQyac0jnYsQ7Qds68ga5503ZXfSHEzRHEy3OYXDbBqr6ujPmkMUMJKQeJT1S3JShYzWo2jayAlS7fptlQNNROimewb3IFIu0uPo4nHXcK3d5mC63xxMGy5ibwyzaqyqo38GD+39SSq9cQghOzADsJ8jgBZARNO+nf3jYau2UI1S6Y0DiLuPMPHwDMIh7847dS8Oi3oMujabuMNWLaJSRlkKZQILOQdyyrvNwTSSqHqOYK9ZH9KaWB6B2RhWcjMWfpp2RoCouCGhs4cQGin3AXxW6Y33K72xNHKmaLDrYQ1pFmzS+Z/S2q4l/t2gtX5Z6Y1dShKT3IfbnUKbyjGPoWoY6+g5Y2i5tWY93XX4GFrOMCvDqm7GBnDTqzn8dPJ8F/rTpi7KImfNDVJdDuIItdIbT2B3x/LhvFOf6D45sX5fzVU7OJmUdQghZxx6sLuLxB2NJ5XLPsQF4zLZLLO+EMM4sZKOHnCrfPnsZz/Hyx9+MD3sYN6pL6UKPDn7i8NWbVbpjSO4SxPfnnfqUd6TJJVLN+EuQ9DFoj7PIfxMi4qweOG77sFmH4t/t8fcQcusK6uaoweAB1jsctXGwskDahG0BoQjshVJe5xIbbg6eQB4WOmNoeHs01HxJx7OneYCipnoVeiUMIZZN1Y2ogeOKzq+NDnm08lzl1M+nnfqDcVaGliOaLPYg9j8nFR64zb8O1ppuikmnf7ykbqgSpgniYekHbAWdhfeH09r3YHYl4i5ymWXzLqyqpuxAI436zLLEVM8cDzltUpvLK30GLZqsUhXXnftMwhhtTChlV+EnG6U9Us69x7E+/fI0+jCXYhO3sdk21cZaJfs7cG8UzjL5iOItT5iJ8+sMysX0VP024DI/V4GgDdefeV/XD5/HsG71T+VHPIIYlMwoiqW+5LnmKCM6oHjyB4Qqom5zqPSGxvflRiQudYkpA+v1dSkOL4N8ZpHice2IbphuzY2ycZSl65qaLmBzQgOSpkMs2qsTI6ehMd2IOmK/e77H/7073/zFZ5+/Q0uvvH6P/zL333/v736yiv/C6JrdJR4auBhKdLUDFWGNJDI1TcH0wMI55kV2YYe1qTCZHBJG27DuaN0NQw1Uz0hmQJjp0wXih3JBTMeWm5cfUMln7d8VAMxzKqwEhG95UDvAwDhvFM/dhKV3ngE96lNmHfqSXVJ3aobqaJkkcPKgcW16mAhgRBX8ISqCDnv94pjRshoikoIr2lX9dAxu+zkmU2j1Dn6Sm88c3CEVwA8oY7XGK95WEoh6JZWXihaDE0G3QmZEKQbn7Kg1xRlOXFyxhOaIaBjM4TYqFZ2vtL5okSqTIcJO3lmEymto6fctY9mpIcJZz/xYO8xcJyTNuYMOjIbJk8mB3qHnG0mlFq5p5mWCaHRoEY2d7PEzGLovH2djeS4SzfveQyzjpTS0Vd641343aB8WOmNq/NO3Ud1yw5FprblkJ/oRrZnBZUvNijVIYXSMZHuRuuwVesPW7UtEhXLSg1FMNtLCZCzt0AbzTx4hNlYSunoYdkIlYMPJ/+Ycv6uZYOFqE96ZgfZJZoN2G8kS+3SBdAol5/Qw8lay4xTNswmUzpHb5FT1uUWSQxctTWQKFV0TSkVJXEsw0VQLErvKzQH0yDesLWJkCnvHqVTWHH6xdLmPsQdyMJrbQ6msVa+jcolw6wNpXP0KDbavZWswjHEV6POaWOdrqAc+L1UDrwPtzLM2G4U/0ypnL5LvT1F7JPUw7sAbtvaZJh1oYx19D6mR2Uy79S3aINWd+ZpppxAmcnR1c+FdOGPmoNp/JAXWQOICDx5t2A0RETGsFXbTtn8MNF9bA2llOKN5ENwvp9ZMcro6Aul0htvzzv1fRL9imiwxwjLssCPAPR1lCAduQ3DIdtnzEOdihwN0umU+x4qktKbvBHcB6U3sFgtdAHiTsHJLsOcJhvn6JFyBqTfHqT2BmYOKR4j5p16VOmNi3L0ebo7NhzCz9CVLpYljX3YTSpbOv8Nh63aqDmYpieacTTPrBSb6OgbkOSYXVMcjhzAXuZYxYN5p+5rv+OQ/nsBIpc+cjVIejPbWBxS4sNu7JQP4biXkGAfi47el12GORU20dFnku60VDifPbhpx6cj7RDCefisxun6MJLQm4no56PmYDrykKd/SHX1cQpn5GgvzqU3IMopZ83BdEaDwK0j8ITNixA1+w1Y6uy4QpvWbZzMvd0ftmpFKJ+mz7sN8Xmq0jm9FUzQBT+AeE/7hjIZDVpX3NNR2n20xFr3IfSvrAT5bCmd1k3R2i8Ariejd8o366RODgFsx5t7JBNgU+//QPVF8fTaDwE0fKSeVHozruqPJHK2VPJoo7OTPBaio7abeKwK8aWyKq9UqW/a6Oy4kvM5fVTUWrJkO1zmAtBnaClY0rWpWNfFsjn7jJkZC/6kaMpYXlkosZNvDqZd+rDo5scvAHhKEe02OWvTksuDrGiIBMhuGtpcsD/v1KuenHwIhd5MXMpIX1ZTu0dQN1oZ6ewkbEYAgrRDHrZqs7gSx3KjV9r5a6mzYw29vqzPaSGVaum+BMnvuw7mpXfEkkHwJpSqEZHKklUd/hfgNvfaiI1z9MBxpKo7GFzGyFI7Jff58069D9LTsSDXvg6aejMNGKavyO5tVSrFRGcnYXMHIurOckr3kDOIRWFXuQ4TnR0P5L7PvtdBF7G8VGLo85yES0qs4WsRnuie9QJiyujo7xVo+xk5eVep4gsQEdY23WpehJgcJT0nxC3llu5t5bxTb1B0fx0nG6EyDgDcm3fqW/TP121rhBy9mWGrtk859kmOdg0Aka6BiJCjHLu5OjsJm1WISD7zuRSVB6TPkwtFsjqdvwHKszHrO1evY++yYwS+7uhcoLvFL6Ocm7G7cIu2s+jCb816H8IhzCAcSYDFKCdK5+AqvXEXotqkAXHBeAbhLEbpmn1KM1VpfGE69TAqolKIPnh5EXKSGcT70Mh5XgB9sbJYZycv3aKdf6fN2Ube8xJ5/a6mzXZzMA09NZG54Dt9o1sFFng+76YR4hQi/9JtxgJApTeeYLmByZXrHwWXogLs5naKksaOSZrjrielTSPiC5WpFAGlZLZl7wP9bmQxzKQN4BPV5hyld/ZNK2qaktGEid/tQOxLmK41grg4FlKJozvHwGVz1PacUAzUcbB/T+fzpzj+8VDMbT5zUt3Umfj8u6koY+oGyI/kjPkouBTAv5MHcq7GNKfWtBTzfqU3Vjqw5mC63RxMG7ShvOMjP+uiN0NOU3Vh6tM/U5synR0AC3sINpvOI6jz9fdhsaGn0Nnxwmlt+DpwmgJ9q0So+0SdNKUrpXT0JFHg6yp3AJFDL6r79DLlnxeo9MZVKpe0HUZ+JVluSaqRfYpknkB0gH5M9uNqIJcc/QRum2sB7X8cQxU027YSwXTRGUmqeyJYpgyoDDFM50ap5HPLNgWTqO4JbY7PQDvo8ZUvt6mmYk6g908WVKoE9gqvqS9l6iZJpTduw37Ix3F3qMGtqBXJ269Kb9yHx5zpH7//bu/dt853DA7Rup1Ovic+bh8p+kxKENz10dBDDuz4gulprek+iKs+hMpowzf+2++56uA35TOJH0NdUOBcS674riS7oxew+Xusc+pG9dqoeEHVf6P1um0pZUSfhMoNH1kcej3h5AOvi8rHm5OvnT8HQycPiPm0QdYTCog8geUo26pRSUIRt7Zpm77WmrTj0j0d39LLUiMTqHs4Gi7nzGAE1vjxQaR4PCzypGWsulli3qm3AYCUJv8SwE8ynv7ZvFP/89Rjp9bBSBuv3vjpB+/bHvq0OZhmRdRh8gdKP7mWZ6ad5S1PF9mF6DWdIvJhE8Cuh4tfFX73gULF432IPZEnkt9FcLgwZuSLQ4j9i6U7CZKZGNmec53IKJfcA8R+VnMwvYvllG4R+4fHrISjTzBDtpMHgOuV3ng3JehV+GZHAqcoLsm7b1ZcTXSRvUl6/KX1MYVJUmngnLoguxES76uP23NJP8WObX4+ZXcCf1/aQPH4hBzGIZYjftfNUWlQRKWkI8grSRooTz/BWRMqHo8S/9+HZO+uOZhWXdNuKkqfuomp9MYz6A8Mv1PpjWdUf76y/PHv/a6rCeWXniL92xADP+56qu4IISKXe2Tb+U6K7gjaZO8mgD1PVQrXIF77PbLrvJFJdwSXye5tD3sJoezBxF6C9PeOKPdUMqL2RgHrWFVkF/kHyfcuQ9+msE3ZlYjoK72xKleZxQWIq+g2Cip9S1PgvNtCSEawTaH06CqyNENC7IyaiVyjlD5EvXy81j5Flg1bg3Sh2EumtWw0diTswtMGNJH3mR/JHrR9z6lsVXbOZ6n/Tzsz107ztSAjAIk0TVxrDqZBEUJnqxLR60byaa6Q8y28fIkoXTu4QaQewOH2u0lDuJMOhkoZJ7Zt3hQhh5JUTSxIZ2Mzgkh9hMnHh61aQOWRVneBVG0TnIZscEyGM48sTaqOS36uT+u7tIpI3xtFNZeq1HLkbTUJSu/oPaRfPvt08vy0cvSNUzqPd0jpMTB1oM3BtBrXoCvsVul5Rn/HrA7WYas2olI1067YzC5deg3Gjpres7PSQ5dpLN2wTMXJZA8eDRe109nRS6BgRnZnIxUopDtU2QS4QjZlS+/o4adipuHBRhZx+WcZuwRNS+IeGObAo7wnUH1wZGh3Ker2QF9jw7Vt0jBEF497w1MeJJFAdV5flWaj1M9ncTFbBULF41HGMdK/XRGdsqvg6ENPNq57sCPjcWLog6k+feGYRpmklz/RyVlT1BhpVuyE0NgraQ6mYZwGynuuidZ83KWrYXMG0Y2be2dDFwSpZr0rGRH5QoRIfy+ZwqnRUBzV+dKpKFVT2Rn0qpQNWST+KCuwyAgOvAcNq+DovdzK0K63rc57Ft3E/3uNdj7/6muf5kxoIMdR0Be7rxvJkoMINVIKuxBVK7rkas1Trv+O7kUvvq3WiKzaKK7RpaF4XOZofTgG1zuAla5wcyHjc2L7d7nm+8K5Co4+S4/diALaow9SJWeRT+NPfm27B32MasMnE02t+REM9WboorCjyq1TNY3RhmZCaz5S2KxC5PqNSh3zhMqaNPbQh2yCgobicZnzcCoCIKciu7CbfPdClzWsOKpN2EjjWNX8jZHtYmSsgqP38UVKRnIPPNiLSUdBI4+2AQDf//CD7aGPPTQAzSD5EDf1pjqpCKFuYNu2sUnHtBWiXn3YO8JGRrdsUQ4+RlWyuPT+qN4znZQWoXqeSUQaGDx3baCLpEt5aaR43Oum7Co4eh/lat34fyinaZIakHEA4MN0veu8U59In+3AX/3s5/jbX31uetihj7sXypN3kw6DqmEC20hWVt3TFLLL1oPByW4VIrI/tkHn6NpWw9BrbFC3a2wz3kNo2K7VgWeG77vud0elsGpygbxiW5q64owUj2sFlFk18z43ZUvv6EnUzIXb6WHZw1Ztl27lTaP7ZwCuU734RHU+izVmMv3mBX725SyAvBwrySFJ7Xr7gFBq6hPa9DyC0IH30S9wPWHzCTz0INC6vkzY/dBVg4VSOFHC5kMI2euzIMr4nbRUz0G6+G7GBVK11+Wr0meVkEXezwy/IyrRRm+bsivRGQt5N54W6fF8SYat2g5VY+wjvzRSSwJ13qlHld7Yu/b9//z3/+IZxGbmDPJ86gFOp45/x5OoWDf9czJy9sSup47X9Jd2B2cz+DnrzqQPuc5SG3Z3xVlOZgR5umKjIvqMiNv0/e5DrnjrreN4JRz9vFMPkkM4DLiZfoCqProQH8qkc38E0UzjwzHchr9BJwsOnCIFHxG1LTMfSoXk1Bc+yJ7sJn90XqviyzxxsalxzkDxK6XzHbZq/dRrj8l0FqqIP6cNfwS5uJnvubVlR7UJa+RDhq1aRH7JmyBimtKnbhKYpkRuJ9M+1MHZhRiMcQ3LEfwNAPebg+m+q1wt3UX4SuGE8069TE0qXU920l8GX3a92qT0RZR6uOjmKGlkXIQGCuzSLRPfi1g1PGzCplE1TzV8GF8ZRz/v1CMaL3gP2WVfz+ad+lacsqGNvgmEXo7OsN4rAB5STnbksl6IJi2X8tCL6f2F04Yuelcp978FkWKJPJgeJWxehIdNd1rXxYTdp64X7VhALWHzJuy1l3QJLY+TNuypNknpvZE5q8xBP3TBKV1z4CkzUjxuMyQps3nKx6ZsqVM3dFu5jZPSrdlHwaWubicifZBdUijXTKtBmgnt9I+CSwDw+Iuvv/nLv/n8i38LvQjgEEBQhiieLpCjZJkmVcyETcthExQJ7SffU4qaqy6VN7RRejMlqrbVHEyj5mDat1RzbENU7YwSNvsAtkjfp6ictG0KJIR6GIlsraqLa6hxrj7k2jibgmrPMHSwKduLvADxXjcc7JbT0Sfy6DLHeKM5mD4btmpBjo0QfvLkF5pCvjczsqa00A6WU0LX3n3r/LWP3jr/7NPJ8wdQd5we0vH9pJMn9c1G6rlREaWcEmaQ7AdQTtFW1rgPdQ16v2khsUsXjweKqGgH9l+UKOPC462yKYlLieJQPYxE5ZClBQia7/+ZByJnRVaE7Shs14fcPziniErn6Jt6E3ouUwQnHYJNfwiflS9PmoOptOrG4IJy+aPg0h2IgRRR1hNpHGHWxszHld44/v/H8059aV0uUCT7SU43aQDhsAMDuyGE7LDU0Q9btZAuIJFueRrdQTVUF/6hmIzUpdp37e7YJskOq36f6AW47mMTOYFriWII4BMP68gjgnxKUuihUa/sqNIsTl38VAXYQAF3SqXK0VM0Y1JGeUGRg/VROZNGdVU1vaA8VOWNK71xtdIb78Js9/1apTce0TxdX0TIaSpLdKOGOgbpb/tQo+GnDTNBrjZynCM54ru60TLdnTU0orMH8L8xq1qjbk58pPOkjMoemezxEhnvjcruOqHyBZEH21IbrpuypXH0VO8syy/mIXO0hZQppZ2aSrNFg6U1k+7+lzBUHSSuAXha6Y1Dy/Uc0zQYoJHoHNW5Xd3ViaiHCa35vE0oE70Zej1R3vPii4HOXgHddSh1dixR5ee1LigZcgjpv6fq7+tauhs6Hr+qmDZJScn43jltypYpdWPj4ACICCzeoPXZNixhB4vOwvoWK3mLS05+5LIw4iHcowqdSPYYSrfk7V9EsEtJ5B03MbTXTn5W0tBnR1d2GcBxauhWczAdFZyymDge38j5GUBm9YcuhQzOKAsZkfW+r1JIBRcgvguRzcGliOg9OOePE3oskaOtLK7EDSYedD0eNgfTbZqH+wSehpbYDkVvDqaBbdXLkHThFXaPICp3jDapyNlGsi9P00CzPmVzAmCWcQcygUVESncqjYKDDBMHLJP2OP7sErLPm6sGFIDlO991gf6+qr/DDYgeHR//VFjvO5bC0cNPs0yX8o5Fd+fFXxYf+wC78F+TfQF2KbARHPKrQ4msMZUgbtlGuhRddpN3DE1L2eGEzV2IdMvCF9YkDaSwG0JzYIsFjw3vsnYg3xi8D6jz8566woH1Td/0Uc4pcrmUJXXjox75Ck5nIyi+NfURvRUZAeZCEV5AP9rKDieZQeQSZzCsyMmgDXFb3CX7uZuveVC6pUFOOYC4yNnutyRpA/iMpAgm8DdH1sbGCOqgx7WyJ+Yx5BuTgSf7ZcNnJ6wVtv0rZYnoVxHnEqj/Mzv8Ix8LkVHpjTNv9clx3ofYG7ljG8mm6EN8GW5AdCE72yRHOSJ79wFc8+Q89yFe+w2yO/Jk85Ds3ofh3VpGysMmb66yBchliW1KA0eKx9c6T3/GWO2hlCWiX0VU0Yw2T7/+7UtPa5Fxo9IbVzM6bBvJHyg94upA03dm11xkJJJ2kj8UYRNClbPhaLMKt1v7QPH4xNQQ3bUsPU4XeBmJ7Pn6AAAUBElEQVQ2DmRiccxKUvBGqwlWn6+yOPoI7rdFezidbr24zjiOXq34+sUL/Pbly7f8LEnJDtT7H7tIrN9HO7+keeyerlxFjt0uEjpFOnLRGjYjLJbhtj2pZ45g/7kIZQ96bshS6T3ZlAb24bcxsZRkbcLa7hVpnncfksxBczANTDvSy5K68dF0MvKUfshjQv91WvPnL+buK8mnofqFhzI6HXztQZzGBbwIVPNAVfhOeWg1P8FwszeGjvE207nEtHE2m7CR4vHQ1FApHL2HnOu9RGWHlxKxDHYAd8nY//ftt3nTogonVmSkipkjl9vTZDVMQulx4lqG2jwZXZhc60J1j4XNbSTUM2mtO67lkc2T0YWx3a7BsaHLuRXoRuku55YGDCVKdfhAddeieyG1IqMK6mPT97cUjp64CMs3LvmF8lgiJuNe6q7BdqTc4eF3/zjxsB5vkLPrOpiYUIlh0uYuRC281d0DpVcCScdhA5b5Ybod3k2XfA5btTbEhck4hdEUsw7iUtKRzbpQQEki3bXlyuY6Bi2qv62vyp4y0zjDcxt9p0rj6GPtFItDvc9ozWDhImJ563qIs/2AZNG2aeenaFT1t2vD4vWSzVuyuz1yTO1Ek5wJVajXugP1oOwsIotj0gQebMgoOkWnuhv3UTJ95mR8xg5c7+o1UV2ojVJJW0dHNhP6ioNun3XK0p5BbKAt5eXp1tymaSiLA9WGZXLNX794cfz4W+fOpZ/6iCJHkAjZU89rTHM7a2auDHKw+7r7HRQh76ejecnzZhCpnVy79H7uatjsQ6hhaqX+dBUs6TVpSUHQLXTVdc9D1VkMzVnFlraBxGeyAPsXM7R3VMdobeDnvCYbpO8DfW5lTlX52nyTsYYPdS82pYnoY+jNuw71pHlACAgFKqdR0KZsqPrFp5Pn7b/62fSXn06eY/zLz4//fTp5jr/5v7/Edy9fAqkPEunJF5rjg0VtOKU0Rob5ap10Rx/6kW8/z8kToa5Nej26+yJVaHQ+U4dpv+CN7UmBtoFiI/5ViupvKPLeLpr9vnBOj5WlvHIBynM20o+TLkwXwHal93xED4/mnXpXYmNLUkJnux5pFFjpjY+vtN//IA8wvvj2Ozye/gIAblR6z0cp7fg2/N95xNx1GE4SQDgYpbNv6mnWH0PiZ0GWqBjZPYK40OvYnDXFBKnMSD0WVdPV8RmeTNHKGzgzQvFdoD4csWwYCSDuUiMP9lU04KcR7bRoQG+9p1ppRN8dmR+7D00pltJF9CpoGMcEouvwWuLfx5XeeFLpjbvpYzSjwjyW9gAqvfF2pTfeh3nJ1bXkOgueBxvZHjjU05qPYFg+SLeZSQG6BSjldtdkQ5Oi6Qcq/RbiFgz3CeI7G9XvadPWh2xEHj7sqy4WZ5W/933MaXMapcleKb2jp2EcRxBfVpVjvQzh8I/SDp8ivQ8tTv0AIg8XpdYTQkThthII8Trj6NJqmHAOz1xnzsZ3VTIJ4uaJZn3Xwu4WgFCRGopsqqaoKmeU3kimapgZVcPYXFSlWvO0dmXq0BJZlLjno1lKFfD4aGYjZGnWZzl/S5nCpknVnO/h5Hu674enANIUWdpRu0S7dJuxSTRG6qlQbkJSJKbS+hghY3MvmarxxL15p96lC9kxV9+7iHfOvYk3XlvOrP3syxn+7vCrLJsP5p26TZejFIqy2xC3iFWIDeSbHjYfZxBpjziP+5lrl2G8kQwRFc4g7gADl6g70e17FScVO6FuGsjwXAFO3uddn7l/uji1IfY19iUlq672Q4i/ZQDRo6DlsElYbhviIh/5XJMvyrTGhDrqvslayu7obRd3OO/UM7+I5MCOn6MTOTmsR8m8U9+iu4SHf3ThbfzBhXfw6ivZN1pfvfgW06+/xvSbF0u/m3fq3luyUxUOvmQN2licberLbnKtV31E3ZK9Hi92Gea0KK2jj52fg4kPHTYjl6j0xhMUo8p3+6Pg0g4sU0HJCL8IJw9IS9lMW/tltLH8mn3YTWq57LneZlMkPMHinZyzXYY5TUpZdUO43lru5NlIllNlRfR00SlEevW9Nyv/CcCPbI//8cUq/uGrb56//OGHv/C4rDwaHmwEBdnNO4cp21hO1/mwyzCnRikdfaU3lkV7ptyBxNGrmg8Skq57AHZSed1CFPrefuN1/Ovf+11rJx9z/cd/cAnitZ5GNcBtH3lKSVObL7vJuw+nRiBABADNwfQ2Tj4Dhz7sMsxpUsrUDVXOqORUTbg+79RHQGZ3mYpjx1NEbv5PfvQB3j73pm+z131UaSSh/PTxhY9keKuqLmEDu5NhqxYkfh65dIBSimWUXBf9zSPfG48Ms2qUvrzSB+QETKtlHhapwFeAkwc8D0ZX6M204Sg/rNDGsdLZSSA7tg1xZ8cwG01ZHb2vioZ9ciq2A7izJrJbUzu/pIHjC2/7CE3FEO5hqzajztEjGwliirKXOk7pYjKytNmGiNwXjh22aqOhB1ljhll11tnRx01DXRcjRUT1tbeKGyxlqegoI09v5gHs2tuVNdw2OjtUex7l1JzPsILdjAzji1I6eiqLdC21ix2ea5Tr1UHUzp8rKm0TY9xZmiYeoJH1HHLW0s5Rhc2QNGkaOU8NYCbkNUJOFQxF+l2PF0GGWSlK6egJJ4flUUfG6wix6htv+DQn43KO7ksmJnozlG65pTkd6SE0Zgdo6uwAMNObodcT5a6SYdaQ0jp6SrtkSRVncQ8o5ziz987/zmmcxiVyNdKboRx+Iyvdkpi+FGnaHEGhs5OwaaM3E9hOu2KYVaa0jh4ASNLXdFzfnky22BFvwmMy/ZoCMN54bA6mbUqtGG+GUi5/orB7BIsxeWSz3RxMuxKbEcToQqOySYr8dwsYWsEwpabUjh44juylSncSbs879TDxsy/J08iTnTITwW1fpJFOt1Aa6LatLkyGrLGx7HDC5gjAPZvqHoZZVUrZMKWCxu/tQmy+xZ2zewB2VLK8PqK3Yau2VemNG/BQbllQo1QarcYpSo3EWi7O2uoUaTcgNsAPgOONUCdiWWT6sZpstHKwGV98roAmlrnaZJiyUkoJBBVUjXPalROP6NyjSm/sbOzLb789DUefG0FTRBtfLC97GqCxixOVxyvwN7i9D/8yFCOcNFNdbg6m7YJHAjLMmbFSEb0NlOO1llNINgxVeuNtCAdhXYnz2iuv4PqP/8D2cB0e6OSuScrgWpELWTGch3AzTFlZqYheBd3a30g8dCwjO2zV4vppG5G0hak/8059n1I41nNeX/7wA2bf/BbV4qpvIs3nTbDo6LUnyquQ6LZ7SYlQmuX47+c6oIRsmmofMczKUvrN2CziJhwsOnlA1HYfxXlYyhObDvS9LZsiRPX5F2FfiXPzr//dPytENx4AdDc+6UJ4AOAZ/ddH6maGk/f5AGaNT1lMyN5j+BvMPIF47YC4IDU82WWY0rGyqRuDiOw4uqfa6zzdm0cQao0TnXXQkPAr5157Fe+8/jqqb7x+/Ltfzb/Di5cv8eLl94cA+smKIBoJ5ltwyzoqp/ezazOzlY7fhtDG2U081oCodY9sbJKNI6RGF9JaG7bVPPHIQZ4SxWwKq5y60b3tvtUcTLvDVm0ybNVmzcH0Q4g8e1oa4RD5+i5LfBRcCl9+//1fvPbqq0sbjz8++d8Iy52+XYgKFVfd/WMcUy8hxGg/247kfjpNQ1ruM2qYMnaq1OH7QLJJ2qd/QfoYDZvbAB76SP8wzKqwkhG9ZGhFHge6ZX6V3ngHi0Or9yGi8VFqDTp3BzIuJitcPOWKvY22I6dsVBJJdye7sguNTCde02YEEbUHit8HEHdeRk1Trrr3DLOKrGqOPjB8fm7UXOmNo0pvPANwHyKlco3+3QHwWaU3PqLJV7FC5MRwDTH7qWadhqWdYzzPLzXqqqX34o7qbiKhXdM1XEcbGaW0dD6j1BddPFjYjNk4VjWiD2FYVy27VacGrAiGZYZvv/E6rrz3Ls5VnATKFpqaKJ9t2pC1cHfgC8qLX81Lt1C0vqtzoSFHvyPb4JbYnOQ9L/H8fYjIP/N9oIvrto9xhQyzaqxkRG/xZd1LP0ADv5/Copb8q+/+EX/9i1/i73/9hemhST5rDqaT+Id4SAZdkO5CXtXzGGJjcov+eXfytJYt5JRpkuOe6N5NDFu1LvRExSYwu2MLkXN3RReDiJ08s6msZEQPGEsbLETP5OS9dFp+FFxyOr6sm4J50bqsGkbT7ozsdiW/C2FRDRNr4ajWQucMirowMkzZWWVH34Vex+tCyWGlN7bdRFXyZ7//I5c0jlZHJqUeQizmpfcgNjoj25PnnDOEcMrVxGNWm6spu12IKp39xGPaaSCFzT6EM99OPLZNazVW82SYdWIlUzfAcSpgKSWT4lCySehdz+TALYWTmzoih/sEy5uPtyCGmHeLmIlKF5B+yrZt+WXSbhfLqaE+AKMKmhQhljeS4zJMhtloVjaij6Eyu30sligqyykrvXEhL9glhaNK31iUkd4uIrpPpcnu2jZVpWw2sLj57GXtqbU6yzowzDqwshF9DDVCVQFcBXAdIh+vcvKNotbx+VdfWx+bMfpvZGjKt8KjDF/liY2cn4s4B8NsJKvcGbuA5gZeYbfxT379Jf78/O/g1Vesrp0hUsO4Keds3Eh1ChuP1yxq4mWEqZ+dLyCSC+YONmNoDMNksjaOPg+qtClUrfD57DcI3rVKlQfJHyRqnCZcgCg3LGoD8p6sYsYUKnn8JPGQs81hqzZpDqbPcCJv0XC1yTDrwMqnbgwIij7Bi+9f2h6adsq2Tj7G2wWNNmIfAPgQIj0mG+1nQ0Q2b0IMKLnvwSZIMuEmRMMXl1MyDDYooscpRHe//f5720NLqaIYSwakyhO3moNpnzRjrBwp2Y3TSxN6OLLR2ZHBk6IYZpFNiujLzOSsF6BANYQ7hGWZZXMw3YF6Pm1gY5NhmGw2ydGPij7B77z6qtVxZWvNbw6m1eZgOiGZhaW7DXLSfdPB67RZGqgamIatWpVkjZ2jeoZhTtgkRz8p+gTnXl2bTFgfOROnKD3yIKM0dAHK9fc1ZIW5yYlhPLMxjn7eqUc4GR1XCJeq79gcduB7HS7QRmtXJ1dOTntEOfc8JlguqZTZDAE0SOOeYRgPbIyjJwrTIr/63kWrGnqFQ82TdsjD6oJGUXeYFIDLg6pcRqp0S3MwDZqD6WzYqlUN5tlOAEyoJ4BhGEc2ytHTYO9CeP/tt2wOUw267tqvxOn4CBpRdxraYxgpft2HRVURySyMitDwYZhNY6McPSHTeXfi3TetlCtvZ2xKTgBctFzOVZvNXdpYjRxqz4N0CofE2ELb0X3DVq0NEdl3LdfEMAzWQNTMFN8yxa+98gr+5EcfGMsU6+jQU/eo0fBwE337RLQcp2y6JueS2DuCaH6aQETxX7rq7dOewSfDVm2rOZgGLFLGMOZsnKMHSjV45BnEGLyJ6gm0KakzG1VL1z5ht4GEeqSPASgSRUpfcgkhEn+vsg5rYZiyspGOHvDj7P/p2+cf/eS9d13lCoCc2a9UwvgUAL5+8QIAcK5SSW7+2kx6SuvpxJG4C10s6usfws8GeIQT/Rp29AxjyNoUfpsy79SjSm/ch3Aips76MYDwb8N/NYkfoKgzwEkn6e8B+CNNe182B9NDVc7+08nzLGd5MO/UberOJ6mf+67aMJRqSjr6fZMKnhy7l3OfyDCMlI2N6NNUeuMIIvpUCYIdAgh1nCqVBdoIiz0DsJ10uJqDUg4BbM879YnJyRKbp1WI2nmnqiRyyIBottqHkDoIHG1WISp6RhAX0l0fFw+G2SQ2NqJPM+/UQwCo9MY7EI6vQb8aARjNO/WRgTlb9cjLEOmPHVpLYHC+CIbCbcn5rM3BdL85mDYco/oACS18ytm7sgscN2cxDGMBO/oU807dqSNTs0s0izs4mZ361OC43NmzOYRw0LGPm6KSjw1bte24Esey5LMBMdw7tFkTwzCCTayjL5rCum+LhNI2oU0UHuvYKH59FxZKl7QB3S+b4BvDrCLs6P3jPPTDU8rDGKrc6ZocQ+WfE1XUTR2uQXMwHRkuZwSWLWYYL7CjZxYYtmqN5mB6RFVEmZC+zURVLZSwOQPQ1ZE1bg6m283B9GjYqhU595ZhNgp29OVGpYVTNLeR02OQkB3WSstQpcxdDa35PtxF3RiGScCOvoQkygcDg8MeeDx/RJIDo4ynTWBe5bMLMTJQapfSQA3efGUYv7Cj9483zft5p66bujiAu+KljG2ZLjzpz7QtdWfaAFRR/R3WsmEY/7Cj90/D8fiFdM28U98CcD3j+XvzTn3b4KKgDeXeJ8l0CzVFtW2bloat2iQeGZiwWaXB4CxtwDAFwJ2xBWA6SzWFVAiMtHnaOJFreAagXaTGfgw59xGoIQoiveIqlxBBdNDGF5GqzlQrhmHM4YapYrgK4RhNSy0/VKUuaBRi5LIoB67gRC75sadqmAmAjxM/P/Zgk2EYCZy6KYBhq7ZPaQ/dfP0BMpx8ybimOxA8h3RjmWtnL8MwCjiiL5ZtCDmDjzOec7hiKYvHni5Iu1gs4fQ++YthGAHn6E+RZMfrKiowNgfTqs8mJqrF317F94JhVgl29AzDMGsO5+gZhmHWHHb0DMMwaw47eoZhmDWHHT3DMMyaw46eYRhmzWFHzzAMs+awo2cYhllz2NEzDMOsOezoGYZh1hx29AzDMGsOO3qGYZg1hx09wzDMmsOOnmEYZs1hR88wDLPmsKNnGIZZc9jRMwzDrDns6BmGYdYcdvQMwzBrDjt6hmGYNYcdPcMwzJrDjp5hGGbNYUfPMAyz5rCjZxiGWXP+PyMDbN4O39M1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8e431a438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from deeplib.mila import display_logo\n",
    "display_logo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_tMgB2wTQTF"
   },
   "source": [
    "## Module utilitaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBgc1wsCtAGU"
   },
   "source": [
    "Premièrement, installons pytorch et certains modules nécessaires pour compléter ce tutoriel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0iF-E5xOQRe_"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "torch.backends.cudnn.version()\n",
    "\n",
    "cuda = True\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "WXqWpBdaQm0-"
   },
   "outputs": [],
   "source": [
    "def normalized(x, y):\n",
    "  '''Cette fonction normalize un le tenseur 'x'. '''\n",
    "  x = (x - torch.min(x,0)[0].view(1,x.shape[1])) / ((torch.max(x,0)[0] - torch.min(x,0)[0]).view(1,x.shape[1]))\n",
    "\n",
    "  return x, x.sum(dim=1)\n",
    "\n",
    "def standardized(x, y):\n",
    "  '''  Cette fonction standardize un le tenseur 'x'. '''  \n",
    "  x = (x - torch.mean(x, 0).view(1,x.shape[1])) / torch.std(x, 0).view(1,x.shape[1])\n",
    "  \n",
    "  return x, x.sum(dim=1)\n",
    "\n",
    "def data_set(N, T, interval):\n",
    "    ''' \n",
    "    inputs : \n",
    "    N : nombre de données \n",
    "    T : longueur de la séquence\n",
    "    interval : l'intervalle dans lequel les nombres seront tirés pour générer les séquences. \n",
    "    \n",
    "    outputs: retourne les séquences (xx) et les cibles (yy), en format torch.tensor \n",
    "    '''\n",
    "    \n",
    "    x = (torch.Tensor(N, T).random_(0, 2 * interval[1]) + interval[0])\n",
    "   \n",
    "    return x, x.sum(dim=1)\n",
    "  \n",
    "def print_sequence(x, y):\n",
    "    '''\n",
    "    x : Une séquence particulière, i.e dim(x) = 1 x T\n",
    "    y : Cible liée à cette même séquence, i.e. dim(y) = 1\n",
    "    retourne une série de caractères illustrant la séquence \n",
    "    dans un format convivial. \n",
    "    '''\n",
    "    n = x.shape[0]\n",
    "    for i, x_ in enumerate(x):\n",
    "        if i == 0 : \n",
    "            string=' ' + str(x_)\n",
    "        else: \n",
    "            string=string + ' + '+str(x_)\n",
    "\n",
    "    return string+' = ' + str(y)\n",
    "  \n",
    "def adjust_lr(optimizer, lr0, epoch, total_epochs):\n",
    "    '''\n",
    "    Cette fonction diminue le taux d'apprentissage suivant une fonction \n",
    "    exponentielle avec le nombre d'époque. \n",
    "    \n",
    "    optimizer: e.g. optim.SGD(... )\n",
    "    lr0 : taux d'apprentissage initial\n",
    "    epoch : époque à laquelle la mise à jour est effectuée. \n",
    "    total epochs: nombre d'époque totale\n",
    "    \n",
    "    exemple : \n",
    "    \n",
    "    new_learning_rate = adjust_lr(optimizer, 0.01, e_, 100)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    lr = lr0 * (0.36 ** (epoch / float(total_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def batch_loss(data_batch,model): \n",
    "    '''\n",
    "    Cette fonction calcule le loss d'un jeu de donnée divisé en plusieurs   \n",
    "    batchs. C'est nécessaire pour traiter de grandes quantités de données.     \n",
    "    '''\n",
    "    loss = 0\n",
    "    n = 0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(data_batch):\n",
    "        x, y = model.input_format(x, y)\n",
    "        out = model(x)\n",
    "        loss += model.criterion(out, y)\n",
    "        n += 1\n",
    "    return loss.data[0] / n\n",
    "  \n",
    "  \n",
    "def adjust_fontsize(ax):\n",
    "  '''\n",
    "  Par déformation professionnelle, j'ai le souci de préparer de belles figures. \n",
    "  Cette fonction est un clin d'oeil pour mon ancien superviseur de thèse.\n",
    "  '''\n",
    "  for ax in ax:\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] \n",
    "                 + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "      item.set_fontsize(14)  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51tYmwJ2qoqU"
   },
   "source": [
    "## Objectif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDMnGX4iqoqV"
   },
   "source": [
    "L'objectif de ce tutoriel est de construire un modèle capable d'additionner ou soustraire une série de nombres. Ce jeu de données est facile à générer et nous permet de tester la capacité de plusieurs algorithmes tels que,\n",
    "\n",
    "* RNN, LSTM \n",
    "* MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jS8xrTCXqoqW"
   },
   "source": [
    "Le jeu de données est constitué d'une séquence de nombre de longueurs *seq_len* , à laquelle une cible est associée. Dans l'exemple ci-dessous, la i$^{eme}$ composante des données est explicitement détaillée (voir la Fig. 1). \n",
    "\n",
    "\\begin{align}  \n",
    "\\mathrm x^{(i)} &= \\left[ 4,-1,15,24\\right], \\mathrm x^{(i)} \\in \\mathbb R^{d_0} \\\\ \n",
    "\\mathrm y^{(i)} &= 42, \\mathrm y^{(i)} \\in \\mathbb R \n",
    "\\end{align}\n",
    "\n",
    "Il est important de noter que chaque composante du vecteur $\\mathrm x^{(i)}$, peut être de plusieurs dimensions, c'est-à-dire $x^{(i)}_j \\in \\mathbb R^{d_1}$ où $d_1 > 1$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QzmAxECSqoqX"
   },
   "source": [
    "Ce jeu de donné peut ainsi servir à entrainer un modèle de réseau de neurones récurrents (RNN), tel que le _long short term memory_(LSTM). Dans ce cas, chaque nombre sera l'entrée d'une couche cachée (_hidden layer_) de dimension $h_d$. \n",
    "\n",
    "Comme la cible est un nombre réel, il est nécessaire de rajouter une couche linéaire au modèle pour \"ajuster\" les dimensions (voir la Fig.2). Cette notion sera expliquée à l'aide d'un exemple détaillé ci-bas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDCPei0QqoqW"
   },
   "source": [
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.001.jpeg)\n",
    "\n",
    "Fig.1 : Jeu de données considéré. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ok4sQtHUqoqY"
   },
   "source": [
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.002.jpeg)\n",
    "\n",
    "\n",
    "Fig.2 : Schéma d'un réseau récurrent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCnneiTsqoqY"
   },
   "source": [
    "## Réseau de neuronnes récurrents (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAisYYriqoqZ"
   },
   "source": [
    "### Générer une couche LSTM\n",
    "###  __ = nn.LSTM(input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u93VR8oVqoqa"
   },
   "source": [
    "Une couche LSTM peut incorporer plusieurs paramètres dont certains régis par le jeu de données (*input_size*), mais aussi d'autres paramètres essentiels pour optimiser la capacité du modèle, tels que *hidden_size*, *num_layers*, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FgCALpkgqoqb"
   },
   "source": [
    "### Entrés d'une couche LSTM (*inputs*)\n",
    "### __ = <font color='red'>LSTM</font>(*input*, (h0,c0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXaWaGleqoqb"
   },
   "source": [
    "En plus des données (_input_), il est aussi possible d'initialiser les tenseurs h_0, c_0 définis comme les *hidden* et *cell states*,  paramètres essentiels aux LSTMs. \n",
    "\n",
    "__Dans le cas où $h_0$ et $c_0$ ne sont pas définis, le module LSTM utilisera les valeurs par défaut, i.e. 0.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrRFyn_Tqoqc"
   },
   "source": [
    "### Donnée d'entrée (*input*)\n",
    "__<font color='red'>input</font> =  torch.Tensor(seq_len, batch_size, input_size) __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKuc_SyMqoqd"
   },
   "source": [
    "Il est nécessaire de réorganiser les données d'entrées (input) selon trois paramètres : \n",
    "\n",
    "* la longueur de la séquence (seq_len)\n",
    "* la grandeur du lot (batch, i.e. batch_size)\n",
    "* les dimensions des entrées (input_size)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dh9ca_HMiKuC"
   },
   "source": [
    "### Donnée de sortie (*output*)\n",
    "### output = LSTM(*input*, (h0,c0))\n",
    "__<font color='red'>output</font> = torch.tensor(seq_len,batch_size, hidden_size x num_directions) __\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGShtHdArgvw"
   },
   "source": [
    "Les dimensions du tenseur \"output_0\" sont déterminées par: (*seq_len*, *batch*, *hidden_size* $\\times$ *num_directions*). \n",
    "\n",
    "Dans le cas qui nous intéresse, num_directions $ = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvtzfm1qaj92"
   },
   "source": [
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.003.jpeg)\n",
    "\n",
    "Fig. 3: http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9P1NDwZqqoqc"
   },
   "source": [
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.004.jpeg)\n",
    "\n",
    "Fig. 4: http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-bi9ydcqoqq"
   },
   "source": [
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.005.jpeg)\n",
    "\n",
    "Fig.5 : http://pytorch.org/docs/master/nn.html?highlight=lstm#torch.nn.LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Maj8vgFlyQlo"
   },
   "source": [
    "# Exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOdqVEOp9Zbz"
   },
   "source": [
    "Créons un jeu de données $\\bf x$ composés de 100 séquences de 4 nombres entre 0 et 100. Les cibles $\\bf y$ correspondent à la somme de chacune de ces séquences. La fonction *data_set* située dans la section *Module utilitaire* s'occupe de cette tâche fastidieuse. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "14lZPQwTQek8"
   },
   "outputs": [],
   "source": [
    "# Création d'un jeu de donnés \n",
    "x,y = data_set(100, 4, [-100, 100])\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "4YwIajCYJ5vx"
   },
   "outputs": [],
   "source": [
    "sequence = print_sequence(x[1,:] ,y[1])\n",
    "\n",
    "print(sequence)\n",
    "print('Dimensions des entrées : {} x {}'.format(*x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTKwBXDmqoqk"
   },
   "source": [
    "__Q1__ : Selon l'exemple ci-haut, quels sont les paramètres d'entrées? On considéra ici qu'un seul lot (*batch*). \n",
    "\n",
    "* batch_size = \n",
    "* seq_len = \n",
    "* input = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RÉPONSE: \n",
    "x_ = x[:,:,np.newaxis]\n",
    "y_ = y[:,np.newaxis].type(torch.FloatTensor)\n",
    "\n",
    "# ...\n",
    "\n",
    "print('Dimensions du tenseur x = {}'.format(x_.shape))\n",
    "print('______________________________________________')\n",
    "print('    ')\n",
    "\n",
    "print('Plus en détails: ')\n",
    "print('_________________')\n",
    "print('    ')\n",
    "\n",
    "dimensions = x_.shape\n",
    "print('batch_size = {}   seq_len = {}     input_size = {}'.format(*dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAJ9tNujqoql"
   },
   "source": [
    "__Q2__  : Donnez un exemple où les dimensions d'entrées sont supérieures à un, c.-à-d. *input* $> 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Y4AW07Jqoqq"
   },
   "source": [
    "__Q3__  : Construisez votre première couche LSTM. Nous vous référons au lien dessous la Fig. 3 pour plus de détails et exemples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0zyJ2FFBJ57d"
   },
   "outputs": [],
   "source": [
    "# Q3\n",
    "# les paramètres\n",
    "input_size = 1\n",
    "hidden_size = 6\n",
    "num_layers = 1\n",
    "\n",
    "# lstm  \n",
    "#lstm = ...\n",
    "\n",
    "# forward \n",
    "output_0, (hn, cn) = lstm(Variable(x_))\n",
    "\n",
    "print('Dimensions - output_0: {}'.format(output_0.shape))\n",
    "print('Dimensions - h_n: {}'.format(hn.shape))\n",
    "print('Dimensions - c_n: {}'.format(cn.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MX0t3nT98kAn"
   },
   "source": [
    "### Réorganiser les dimensions de la sortie (\"<font color='red'>output_0</font>\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIVvInzN8zqm"
   },
   "source": [
    "Tel que mentionné ci-haut, seule la dernière composante du tenseur \"output_0\" sera considérée pour prédire la cible associée à une séquence (voir Fig.2). \n",
    "\n",
    "Exemple: considérons la $41^e$ séquence ainsi que la couche associée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "w50E_nbBJ5-e"
   },
   "outputs": [],
   "source": [
    "print(x_[41,:,:])\n",
    "print(y_[41])\n",
    "\n",
    "sequence = print_sequence(x[41,:],y[41])\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "F9ZExe-iJ6BE"
   },
   "outputs": [],
   "source": [
    "print(output_0[41,:,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vat-3CyN9j9y"
   },
   "source": [
    "Tel que mentionné ci-haut, seule la quatrième (dernière) composante du tenseur output_0 qui doit être considéré (voir Fig. 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Jt5sipkq97_b"
   },
   "outputs": [],
   "source": [
    "output_0[41,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDD1ICcJL_B7"
   },
   "source": [
    "Le tenseur $h_n$ regroupe la dernière sortie (*output*) de chaque séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "7rh_45NWMbqT"
   },
   "outputs": [],
   "source": [
    "print(hn[:,41,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ewppj5yvsVJT"
   },
   "source": [
    "__Q4__  : Proposez une stratégie pour transformer le tenseur \"output_0\" de dimensions [100,4,6] à un tenseur de dimensions voulues, c.-à-d. [100,1]. Spécifiquement, quelle opération mathématique permettra d'obtenir les dimensions voulues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "BaS5Ku1GJ6Dr"
   },
   "outputs": [],
   "source": [
    "## Question\n",
    "\n",
    "'''\n",
    "Première partie: générez un tenseur M de dimensions voulues et utilisez \n",
    "la fonction 'torch.matmul' pour  multiplier M et output_0. \n",
    "\n",
    "Indice : Le tenseur M doit être une 'Variable' pour être multiplié au tenseur \n",
    "output_0\n",
    "\n",
    "M = Variable(torch.Tensor( _?_, _?_ ))\n",
    "output = torch.matmul(output_0}[_?_, _?_, _?_], M)\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "Deuxième partie: gérérez une couche linéaire de dimensions voulues avec la \n",
    "fonction 'LL = nn.Linear(_?_, _?_, _?_)'. \n",
    "\n",
    "L'entrée de LL sera output_0, i.e. output = LL(output_0[_?_, _?_, _?_])\n",
    "''' \n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnWwn0qbAjQG"
   },
   "source": [
    "### Construction du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af7055DXtEq9"
   },
   "source": [
    "Il est fortement suggéré d'utiliser des classes pour définir les modèles dans l'environnement Pytorch, et ce peu importe son architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjTN6N9pB96s"
   },
   "source": [
    "__Q5__  : Écrivez la fonction RnnLinear.forward(). Cette fonction aura comme entrée *self* et le tenseur $\\bf x$ de dimension N x T x 1. \n",
    "\n",
    "Indice : le output_0 de la couche LSTM ou RNN sera l'entrée de la couche linéaire. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "YRLdrWvNJ6Kz"
   },
   "outputs": [],
   "source": [
    "class RnnLinear(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, lstm_or_rnn,cuda):\n",
    "        '''\n",
    "        Les paramètres importants du modèle sont définies dans cette fonction. \n",
    "        '''\n",
    "        num_directions = 1\n",
    "        super(RnnLinear, self).__init__()\n",
    "        \n",
    "        # parametres importants \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_or_rnn = lstm_or_rnn      \n",
    "        \n",
    "        # couches de notre réseau \n",
    "        if lstm_or_rnn == 'lstm':\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, \n",
    "                                num_layers,dropout = 0.5,\n",
    "                                batch_first = True)\n",
    "            if cuda : self.lstm.cuda() \n",
    "              \n",
    "        elif lstm_or_rnn == 'rnn':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size,\n",
    "                              num_layers,dropout = 0.5, \n",
    "                              batch_first = True\n",
    "                             )\n",
    "            if cuda: self.rnn.cuda()\n",
    "              \n",
    "        else: print('You made a mistake, pal!')\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, input_size,bias=True)\n",
    "        if cuda: \n",
    "          self.linear.cuda()\n",
    "              \n",
    "        # criteria for SGD\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def input_format(self,x,y):\n",
    "        '''\n",
    "        Cette fonction permet de préparer les dimensions des données d'entrées. \n",
    "        '''\n",
    "        x = x[:,:,None]\n",
    "        y = y[:,None].type(torch.FloatTensor)\n",
    "        \n",
    "        if cuda : \n",
    "            x = Variable(x.cuda())\n",
    "            y = Variable(y.cuda())\n",
    "            \n",
    "        else: \n",
    "            x = Variable(x)\n",
    "            y = Variable(y)\n",
    "        return x,y\n",
    "\n",
    "    def grad_norm(self):\n",
    "        total_norm=0\n",
    "        for p in list(self.parameters()):\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm\n",
    "\n",
    "        return total_norm\n",
    "  \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        Cette fonction devrait inclure des conditions if/elif sur la variable \n",
    "        self.lstm_or_rnn. La sortie des couches LSTM et RNN seront les entrées \n",
    "        de la couche linéaire. \n",
    "        '''\n",
    "        \n",
    "        # cas où self.lstm_or_rnn = 'rnn'\n",
    "        if self.lstm_or_rnn == 'rnn':\n",
    "            # out1,(__) = ...\n",
    "            pass\n",
    "\n",
    "        # cas où self.lstm_or_rnn = 'lstm'      \n",
    "        if self.lstm_or_rnn == 'lstm':\n",
    "            # out1,(__) = ...\n",
    "            pass\n",
    "\n",
    "        # votre couche linéraire             \n",
    "        # out2 = ...\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DFBy2S82DCS2"
   },
   "source": [
    "Testons notre modèle!\n",
    "\n",
    "Prenez le temps de bien comprendre les dimensions affichées ci-bas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ee-kXlLaDKKV"
   },
   "outputs": [],
   "source": [
    "x,y = data_set(100, 4, [0,100])\n",
    "\n",
    "\n",
    "input_size = 1\n",
    "num_layers = 2\n",
    "hidden_size = 40 \n",
    "\n",
    "model = RnnLinear(input_size, hidden_size, num_layers, 'lstm', cuda)\n",
    "\n",
    "xx,yy = model.input_format(x, y)\n",
    "print('Dimensions initiales des données : {}'.format(x.shape))\n",
    "print('Dimensions des données formatées : {}'.format(xx.shape))\n",
    "\n",
    "pred = model(xx)\n",
    "print('Dimensions des prédictions : {}'.format(pred.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMsFy8V1B0vk"
   },
   "source": [
    "### Questions\n",
    "\n",
    "__Q6__\n",
    "\n",
    "Construisez la boucle d'entrainement ci-bas. \n",
    "\n",
    "__Q7__\n",
    "\n",
    "Entrainez le modèle sur 20 époques avec ces paramètres et hyperparamètres, c.-à-dire $N_{train}$ = 20000, T = 10, intervalle = [0,1000], $h_d$ = 20, num_layers = 2, lr0 = 0.05. \n",
    "\n",
    "__Q8__ : \n",
    "\n",
    "Testez le modèle sur l'ensemble de test. Commentez les résultats obtenus.  \n",
    "\n",
    "Note: L1 loss est définie comme la somme des différences absolue entre les cibles et les prédictions. Pour l'ensemble de test considéré, cette quantité est définie comme \\begin{align}\n",
    "L1 = \\sum_{i=1}^{5000} \\mid y^{(i)} - \\text{output}^{(i)}\\mid \\end{align}\n",
    "\n",
    "__Q9__ \n",
    "\n",
    "Augmentez la capacité du modèle, en fixant $h_d$ = 40. Comparez les résultats à ceux précédents.\n",
    "\n",
    "__Q10__ \n",
    "\n",
    "Finalement, *'standardisez'* les données de chaque séquence. Vous devrez utiliser la fonction *x, y = standardized(x, y)* définie dans le module utilitaire. \n",
    "\n",
    "De plus, élargissez l'intervalle de tirage de -1000 à 1000 et entrainez jusqu'à 100 époques. Comparez les résultats à ceux précédents.\n",
    "\n",
    "__Q11__\n",
    "\n",
    "Il est possible d'activer ou de désactiver l'option dropout de notre modèle en appelant 'model.train()' et 'model.eval()' respectivement. Testez ces deux modes. \n",
    "\n",
    "Pourquoi est-il nécessaire de désactiver l'option 'dropout' lors des phases test et validation? \n",
    "\n",
    "__Q12__ \n",
    "\n",
    "Explorez différents hyperparamètres. \n",
    "\n",
    "* lr = 0.1, 1, 0.00001\n",
    "\n",
    "* hidden_size = 100,150, 200\n",
    "\n",
    "* dropout = 0.1, 0.3, 0.5\n",
    "\n",
    "Enjoy! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnV7bUgZsvz7"
   },
   "source": [
    "### Création du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "c8AsbJZmJ6OB"
   },
   "outputs": [],
   "source": [
    "x,y = data_set(20000, 10, [0,1000])\n",
    "\n",
    "#  standardized\n",
    "xtrain, ytrain = (x[:10000], y[:10000])\n",
    "xvalid, yvalid = (x[10000:15000], y[10000:15000])\n",
    "xtest, ytest = (x[15000:], y[15000:])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "all_data_train = data_utils.DataLoader(data_utils.TensorDataset(xtrain, ytrain),\n",
    "                                       batch_size, shuffle=True)\n",
    "all_data_valid = data_utils.DataLoader(data_utils.TensorDataset(xvalid, yvalid), \n",
    "                                       batch_size, shuffle=False)\n",
    "all_data_test = data_utils.DataLoader(data_utils.TensorDataset(xtest, ytest),\n",
    "                                       batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxVLLPwMFh0o"
   },
   "source": [
    "### Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "srcOUXeqQrZl"
   },
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "num_layers = 2\n",
    "hidden_size = 20\n",
    "number_epoch = 20\n",
    "lr0 = 0.05\n",
    "\n",
    "model = RnnLinear(input_size, hidden_size, num_layers,'lstm',cuda)\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr0)\n",
    "\n",
    "\n",
    "t0 = time.clock()\n",
    "\n",
    "# tableau à remplir lors de la phase d'entrainement \n",
    "loss_train_data = np.array([]) \n",
    "loss_valid_data = np.array([]) \n",
    "\n",
    "total_norm = np.array([])\n",
    "\n",
    "t0 = time.clock()  \n",
    "\n",
    "for e_ in range(number_epoch):\n",
    "  \n",
    "  # model.train(), poruquoi? Indice : dropout   \n",
    "  model.train()\n",
    "  for batch_idx, (xx, yy) in enumerate(all_data_train):                \n",
    "    # Formatez les données\n",
    "    xx,yy = model.input_format(xx,yy)\n",
    "    \n",
    "    # Évaluez le modèle \n",
    "    pred_batch = model(xx)\n",
    "    \n",
    "    # Initialisez les gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculez la loss  \n",
    "    loss_batch = model.criterion(pred_batch,yy)\n",
    "    \n",
    "    # SGD backward \n",
    "    loss_batch.backward()\n",
    "    \n",
    "    # SGD optimized\n",
    "    optimizer.step()\n",
    "    \n",
    "    # stop. \n",
    "  \n",
    "  # Calculez la norm de tous les gradients \n",
    "  total_norm = np.append(total_norm,model.grad_norm())\n",
    "\n",
    "  # model.eval(), pourquoi? Indice : dropout \n",
    "  model.eval()  \n",
    "\n",
    "  loss_train_data = np.append(loss_train_data, batch_loss(all_data_train,model)) \n",
    "  loss_valid_data = np.append(loss_valid_data, batch_loss(all_data_valid,model))\n",
    "\n",
    "  if e_%10 == 0: \n",
    "    # '{} {}'.format('foo', 'bar')\n",
    "    print('N. of Epochs = {}, Train Loss = {}, Valid Loss = {}'.format(e_, loss_train_data[e_], loss_valid_data[e_]))\n",
    "\n",
    "\n",
    "  lr_ = adjust_lr(optimizer, lr0, e_, number_epoch)\n",
    "\n",
    "tf = time.clock()\n",
    "print('Terminé, {} sec'.format(tf-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16ZBNnYKFJCS"
   },
   "source": [
    "### Prédiction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_r19roAVGeSk"
   },
   "outputs": [],
   "source": [
    "# prêt pour évaluer\n",
    "model.eval()\n",
    "\n",
    "# prêt pour entraîner\n",
    "# model.train()\n",
    "\n",
    "xtest_,ytest_ = model.input_format(xtest, ytest)\n",
    "pred_test_all = model(xtest_)\n",
    "\n",
    "L1_loss = torch.sum(torch.abs(pred_test_all - ytest_)).data[0]\n",
    "\n",
    "print('LSTM L1 loss = {}'.format(L1_loss))\n",
    "print('      ')\n",
    "print('Prediction')\n",
    "print('___________________')\n",
    "print(pred_test_all[0:10])\n",
    "print('      ')\n",
    "print('Target')\n",
    "print('___________________')\n",
    "print(ytest_[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfQB8XPdF8wO"
   },
   "source": [
    "### Courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKcZTByAxCjk"
   },
   "source": [
    "Nous traçons plus bas deux graphiques d'importance capitale : \n",
    "\n",
    "* Courbe d'apprentissage, i.e. loss vs. époque pour l'ensemble de validation et d'entrainement. \n",
    "* La norme de tous les gradients évaluée à chaque époque. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "dygg3CwaKwxw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,6))\n",
    "ax1.plot(loss_train_data,'-k',label='Train')\n",
    "ax1.plot(loss_valid_data,'-r',label='Valid')\n",
    "\n",
    "ax2.plot(total_norm,'-b',label='Grad')\n",
    "\n",
    "\n",
    "ax1.set_xlabel('epoch',fontsize=14)\n",
    "ax2.set_xlabel('epoch',fontsize=14)\n",
    "ax1.set_ylabel('MSE loss',fontsize=14)\t\n",
    "ax2.set_ylabel('Grad Norm',fontsize=14)\t\n",
    "legend = ax1.legend(loc='upper right',fontsize=14)\n",
    "\n",
    "ax1.set_xlim((0))\n",
    "\n",
    "ax1.set_title('Learning curve', fontsize = 14)\n",
    "ax1.set_title('Learning curve', fontsize = 14)\n",
    "\n",
    "xmin, xmax = ax2.get_xlim()\n",
    "ymin, ymax = ax2.get_ylim()\n",
    "\n",
    "ax2.text(0.6*(xmax - xmin) + xmin, 0.8*(ymax-ymin) + ymin,\n",
    "         'num_layers : '+str(num_layers), fontsize=14)\n",
    "\n",
    "ax2.text(0.6*(xmax - xmin) + xmin, 0.72*(ymax-ymin) + ymin,\n",
    "         'hidden_size : '+str(hidden_size), fontsize=14)\n",
    "\n",
    "ax2.text(0.6*(xmax - xmin) + xmin, 0.64*(ymax-ymin) + ymin,\n",
    "         'number_epoch : '+str(number_epoch), fontsize=14)\n",
    "\n",
    "ax2.text(0.6*(xmax - xmin) + xmin, 0.56*(ymax-ymin) + ymin,\n",
    "         'lr$_0$ : '+str(lr0), fontsize=14)\n",
    "\n",
    "\n",
    "adjust_fontsize([ax1,ax2])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXLLEtnrGBdn"
   },
   "source": [
    "### Question\n",
    "\n",
    "__Q13__ : Dans le graphique ci-haut, comment expliquez-vous que la norme diminue et converge vers zéro avec le nombre d'époques? \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqPYtKa0E4UR"
   },
   "source": [
    "## Prédiction sur des séquences de longueurs différentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-vKX2NMyF5oj"
   },
   "source": [
    "### Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EA9MAlcqaNae"
   },
   "source": [
    "__Q14__ : Prouvez qu'il est aussi possible d'utiliser le modèle entrainé pour prédire des séquences d'une longueur différente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "xr5OPW0W7lnd"
   },
   "outputs": [],
   "source": [
    "T_= 12\n",
    "# xtrain, ytrain = normalized(x[:20000], y[:20000])\n",
    "\n",
    "xx, yy = data_set(500,T_,[0,100])\n",
    "xx, yy = normalized(xx, yy)\n",
    "\n",
    "xx, yy = model.input_format(xx, yy)\n",
    "\n",
    "print('Séquence de longueur {}'.format(T_))\n",
    "print('--------------------------')\n",
    "print('Prédiction: {}'.format(model(xx).data[0][0]))\n",
    "\n",
    "print('Cible: {}'.format(yy.data[0][0]))\n",
    "print('LSTM L1 loss: {}'.format((torch.abs(model(xx).data[0]-yy.data[0])[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJ1XwmGVKGVa"
   },
   "source": [
    "# Comparaison entre RNN et LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMXnblC77hwv"
   },
   "source": [
    "\n",
    "__Q15__ : Entrainez le modèle ci-bas pour ces séries d'hyperparamètres et comparez les performances des modèles RNN et LSTM. \n",
    "\n",
    "* 'Stardardized' les données. \n",
    "* N = 30000\n",
    "* $h_d$ = 50\n",
    "* number_epoch = 100\n",
    "* batch_size = 100\n",
    "* lr0 = 0.005\n",
    "\n",
    "  * T = 20, 50, 100, 200\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "BXM6MKtmKw7v"
   },
   "outputs": [],
   "source": [
    "cuda = True \n",
    "\n",
    "input_size = 1\n",
    "num_layers = 2\n",
    "hidden_size = 50\n",
    "number_epoch = 100\n",
    "batch_size = 100\n",
    "lr0 = 0.005\n",
    "\n",
    "loss_train_data_rnn = np.array([])\n",
    "loss_train_data_lstm = np.array([])\n",
    "\n",
    "loss_valid_data_rnn = np.array([])\n",
    "loss_valid_data_lstm = np.array([])\n",
    "\n",
    "total_norm_rnn = np.array([])\n",
    "total_norm_lstm = np.array([])\n",
    "\n",
    "# Génération des données\n",
    "T = 100\n",
    "\n",
    "x,y = data_set(30000, T, [-10000,10000])\n",
    "\n",
    "xtrain, ytrain = normalized(x[:20000], y[:20000])\n",
    "xvalid, yvalid = normalized(x[20000:25000], y[20000:25000])\n",
    "xtest, ytest = normalized(x[25000:], y[25000:])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "all_data_train = data_utils.DataLoader(data_utils.TensorDataset\n",
    "                          (xtrain, ytrain),batch_size, shuffle=True)\n",
    "  \n",
    "all_data_valid = data_utils.DataLoader(data_utils.TensorDataset\n",
    "                          (xvalid, yvalid),batch_size, shuffle=False)\n",
    "\n",
    "all_data_test = data_utils.DataLoader(data_utils.TensorDataset\n",
    "                          (xtest, ytest),batch_size, shuffle=False)\n",
    "\n",
    "# Initialisation des modèeles\n",
    "model_rnn = RnnLinear(input_size, hidden_size, num_layers, 'rnn', cuda)\n",
    "model_lstm = RnnLinear(input_size, hidden_size, num_layers, 'lstm', cuda)\n",
    "optimizer_rnn = optim.SGD(model_rnn.parameters(), lr=lr0)\n",
    "optimizer_lstm = optim.SGD(model_lstm.parameters(), lr=lr0)\n",
    "\n",
    "t0 = time.clock()  \n",
    "for e_ in range(number_epoch):  \n",
    "\n",
    "  model_rnn.train()\n",
    "    \n",
    "  ##################\n",
    "  ###### RNN ######\n",
    "  ##################\n",
    "    \n",
    "  for batch_idx, (xx, yy) in enumerate(all_data_train):                \n",
    "    xx, yy = model_rnn.input_format(xx, yy)\n",
    "    pred_batch_rnn = model_rnn(xx)\n",
    "    optimizer_rnn.zero_grad()\n",
    "    loss_batch_rnn = model_rnn.criterion(pred_batch_rnn, yy)\n",
    "    loss_batch_rnn.backward()\n",
    "#     torch.nn.utils.clip_grad_norm(model_rnn.parameters(), 0.2, norm_type=2)\n",
    "    optimizer_rnn.step()\n",
    "\n",
    "  model_rnn.eval()\n",
    "  lt = batch_loss(all_data_train, model_rnn)\n",
    "  lv = batch_loss(all_data_valid, model_rnn)\n",
    "  loss_train_data_rnn = np.append(loss_train_data_rnn, lt)\n",
    "  loss_valid_data_rnn = np.append(loss_valid_data_rnn, lv)\n",
    "    \n",
    "  total_norm_rnn = np.append(total_norm_rnn,model_rnn.grad_norm())\n",
    "    \n",
    "  ##################\n",
    "  ###### LSTM ######\n",
    "  ##################\n",
    "    \n",
    "  model_lstm.train()\n",
    "  for batch_idx, (xx, yy) in enumerate(all_data_train):\n",
    "    xx,yy = model_lstm.input_format(xx, yy)\n",
    "    pred_batch_lstm = model_lstm(xx)\n",
    "    optimizer_lstm.zero_grad()\n",
    "    loss_batch_lstm = model_lstm.criterion(pred_batch_lstm,yy)\n",
    "    loss_batch_lstm.backward()\n",
    "#     torch.nn.utils.clip_grad_norm(model_lstm.parameters(), 0.2, norm_type=2)\n",
    "    optimizer_lstm.step()\n",
    "\n",
    "    \n",
    "  model_lstm.eval()\n",
    "  lt = batch_loss(all_data_train, model_lstm)\n",
    "  lv = batch_loss(all_data_valid, model_lstm)\n",
    "  loss_train_data_lstm = np.append(loss_train_data_lstm, lt)\n",
    "  loss_valid_data_lstm = np.append(loss_valid_data_lstm, lv)\n",
    "  \n",
    "  total_norm_lstm = np.append(total_norm_lstm, model_lstm.grad_norm())  \n",
    "  \n",
    "  ##################\n",
    "  ##################\n",
    "    \n",
    "  if e_%10 == 0: \n",
    "    print('N. of Epochs # {}, RNN, Train loss = {}  ----    LSTM, Train loss = {}'\n",
    "            .format(e_,loss_train_data_rnn[e_],loss_train_data_lstm[e_]))\n",
    "    \n",
    "  lr_ = adjust_lr(optimizer_rnn,lr0, e_, number_epoch)\n",
    "  lr_ = adjust_lr(optimizer_lstm,lr0, e_, number_epoch)\n",
    "    \n",
    "  tf = time.clock()\n",
    "print('Terminé, %.1f sec'%(tf - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "cdmCKBO9jR_d"
   },
   "outputs": [],
   "source": [
    "fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(16,12))\n",
    "ax1.plot(loss_train_data_rnn, '-k', label ='train')\n",
    "ax1.plot(loss_valid_data_rnn, '-r', label = 'valid')\n",
    "\n",
    "ax2.plot(total_norm_rnn, '-k', label = 'norm rnn')\n",
    "\n",
    "ax3.plot(loss_train_data_lstm, '-k', label ='train')\n",
    "ax3.plot(loss_valid_data_lstm, '-r', label = 'valid')\n",
    "\n",
    "ax4.plot(total_norm_lstm, '-k', label = 'norm lstm')\n",
    "\n",
    "ax3.set_xlabel('epoch', fontsize=14)\n",
    "ax4.set_xlabel('epoch', fontsize=14)\n",
    "\n",
    "ax1.set_ylabel('MSE loss', fontsize=14)\t\n",
    "ax3.set_ylabel('MSE loss', fontsize=14)\t\n",
    "\n",
    "ax1.set_title('RNN', fontsize = 14)\n",
    "ax2.set_title('RNN', fontsize = 14)\n",
    "ax3.set_title('LSTM', fontsize = 14)\n",
    "ax4.set_title('LSTM', fontsize = 14)\n",
    "legend = ax1.legend(loc='upper right', fontsize=14)\n",
    "legend = ax3.legend(loc='upper right', fontsize=14)\n",
    "\n",
    "\n",
    "ax1.set_xlim((0,number_epoch))\n",
    "ax1.set_ylim((0))\n",
    "ax2.set_ylim((0))\n",
    "ax3.set_xlim((0,number_epoch))\n",
    "ax3.set_ylim((0))\n",
    "ax4.set_ylim((0))\n",
    "\n",
    "xmin, xmax = ax1.get_xlim()\n",
    "ymin, ymax = ax1.get_ylim()\n",
    "\n",
    "ax1.text(0.1*(xmax - xmin) + xmin, 0.6*(ymax-ymin) + ymin,\n",
    "         'num_layers : %.f'%num_layers, fontsize=14)\n",
    "ax1.text(0.1*(xmax - xmin) + xmin, 0.52*(ymax-ymin) + ymin,\n",
    "         'hidden_size : %.f'%hidden_size, fontsize=14)\n",
    "ax1.text(0.1*(xmax - xmin) + xmin, 0.44*(ymax-ymin) + ymin,\n",
    "         'number_epoch : %.f'%number_epoch, fontsize=14)\n",
    "ax1.text(0.1*(xmax - xmin) + xmin, 0.36*(ymax-ymin) + ymin,\n",
    "         'lr$_0$ : %.1f'%lr0, fontsize=14)\n",
    "\n",
    "\n",
    "for ax in [ax1,ax2,ax3,ax4]:\n",
    "  for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] \n",
    "               + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(14)        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "MILA_rnn_lstm_solution.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "glo4030-7030",
   "language": "python",
   "name": "glo4030-7030"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
