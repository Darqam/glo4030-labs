{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Laboratoire 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from deeplib.visualization import make_vizualization_autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Graphe computationnel et backprop\n",
    "\n",
    "- Autograd\n",
    "- Appeler backprop deux fois (qu'est-ce qui arrive)?\n",
    "- Volatile\n",
    "- requires_gradient true et false pour les variables à entraîner vs. les inputs et les variables freezées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "y = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "z = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "w = torch.matmul(x, y) + x + y + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "make_vizualization_autograd(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fonction d'activation\n",
    "- Avantage de la ReLU sur sigmoid vs. tanh\n",
    "- Exemple sur le vanishing gradient\n",
    "- Réduction d'un réseau à plusieur couches sans non-linéarité à un réseau à une seule couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RandomModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345) # Both Tanh model and ReLU model will have the same random weights\n",
    "        self.layers = []\n",
    "        for i in range(n_layers):\n",
    "            layer = nn.Linear(5,5)\n",
    "            layer.weight.data.normal_(0, math.sqrt(2 / 5))\n",
    "            layer.bias.data.fill_(0)\n",
    "            self.layers.append(layer)\n",
    "        self.nonzero_grad_stats = None\n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        raise NotImplementedError('Defined in children classes')\n",
    "        \n",
    "    \n",
    "    def print_weights_grads(self):\n",
    "        self.nonzero_grad_stats = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(\"-----\\nLayer %d\" % i)\n",
    "            print(\"Weight:\\n%sWeight gradient:\\n%s\\n\" % (str(layer.weight.data), \n",
    "                                                         str(layer.weight.grad)))\n",
    "            if layer.weight.grad is not None:\n",
    "                nonzero_grad_indices = torch.nonzero(layer.weight.grad.data)\n",
    "                nonzero_grad = [layer.weight.grad.data[i,j] for (i,j) in nonzero_grad_indices]\n",
    "                nonzero_grad_mean = np.mean(np.abs(nonzero_grad))\n",
    "                self.nonzero_grad_stats.append((len(nonzero_grad), nonzero_grad_mean))\n",
    "                print(\"Number of nonzero gradient: %f\" % len(nonzero_grad))\n",
    "                print(\"Nonzero grad mean: %f\" % nonzero_grad_mean)\n",
    "        \n",
    "\n",
    "        \n",
    "class RandomReluModel(RandomModel):\n",
    "    \n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__(n_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers[:-1]: # All but last layer\n",
    "            out = layer.forward(out)\n",
    "            out = F.relu(out)\n",
    "        return self.layers[-1].forward(out)\n",
    "        \n",
    "        \n",
    "        \n",
    "class RandomTanhModel(RandomModel):\n",
    "    \n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__(n_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers[:-1]: # All but last layer\n",
    "            out = layer.forward(out)\n",
    "            out = F.tanh(out)\n",
    "        return self.layers[-1].forward(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "relu_model = RandomReluModel(10)\n",
    "tanh_model = RandomTanhModel(10)\n",
    "relu_model.print_weights_grads()\n",
    "tanh_model.print_weights_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random_input = Variable(torch.randn(5))\n",
    "relu_output = relu_model.forward(random_input)\n",
    "tanh_output = tanh_model.forward(random_input)\n",
    "print(random_input)\n",
    "print(\"ReLU model ouput:\\n\", relu_output)\n",
    "print(\"tanh model ouput:\\n\", tanh_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "relu_loss = torch.norm(relu_output)\n",
    "tanh_loss = torch.norm(tanh_output)\n",
    "print(relu_loss, tanh_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "relu_loss.backward()\n",
    "tanh_loss.backward()\n",
    "relu_model.print_weights_grads()\n",
    "tanh_model.print_weights_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(relu_model.nonzero_grad_stats)), [x[0] for x in relu_model.nonzero_grad_stats])\n",
    "plt.plot(np.arange(len(tanh_model.nonzero_grad_stats)), [x[0] for x in tanh_model.nonzero_grad_stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "axs[0].plot(np.arange(len(relu_model.nonzero_grad_stats)), [x[1] for x in relu_model.nonzero_grad_stats])\n",
    "axs[0].plot(np.arange(len(tanh_model.nonzero_grad_stats)), [x[1] for x in tanh_model.nonzero_grad_stats])\n",
    "axs[1].plot(np.arange(4), [x[1] / x[0] for x in relu_model.nonzero_grad_stats[:4]])\n",
    "axs[1].plot(np.arange(4), [x[1] / x[0] for x in tanh_model.nonzero_grad_stats[:4]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heatmap = np.zeros((5,5))\n",
    "for _ in range(1000):\n",
    "    random_input = Variable(torch.randn(5))\n",
    "    relu_model.forward(random_input)\n",
    "    nonzero_grad_indices = torch.nonzero(relu_model.layers[0].weight.grad.data)\n",
    "    for (i, j) in nonzero_grad_indices:\n",
    "        heatmap[i,j] += 1\n",
    "print(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- Observez la distribution du gradient lors de la backprop. Quelles différences y a-t-il entre la backprop à travers ReLU et à travers tanh?\n",
    "- Est-ce que, pour deux entrées différentes, les mêmes poids ont un gradient élevé?\n",
    "- Changez le nombre de couches du réseau. Qu'observez-vous?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Couche de sortie\n",
    "Voir ce qui est vu en classe et faire un exemple en lien avec ça. Idées:\n",
    "- Comment utiliser softmax\n",
    "- Non-linéarité après le fully-connected en sortie (erreur classique, exercice du genre trouvez l'erreur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glo4030-7030",
   "language": "python",
   "name": "glo4030-7030"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
