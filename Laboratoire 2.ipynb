{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Laboratoire 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from deeplib.visualization import make_vizualization_autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Graphe computationnel et backprop\n",
    "Cette section a pour but de vous familiariser avec les notions de graphe computationnel et de backpropagation, plus particulièrement leur implementation PyTorch. Dans le dernier laboratoire, vous avez vu une version haut-niveau de l'entraînement de réseaux de neurones. À l'inverse, ce laboratoire a pour but de vous donner une intuition du fonctionnement interne de PyTorch. Qui sait? Peut-être voudrai vous un jour implémenter vous même votre librairie de graphe de calcul.\n",
    "\n",
    "#### Tenseurs et Variables\n",
    "La structure de données de base dans PyTorch est le `Tensor`. Cette structure de données est comparable au `ndarray` numpy. Le package `torch.Tensor` défini des matrices multi-dimentionnelles et les opérations sur celles-ci. Voici quelques exemples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Création et initialisation à une normale centrée à 0 et de variance 1.\n",
    "a = torch.Tensor(10,10)\n",
    "print(a) # Initialement, le tenseur contient du 'garbage'. Il peut même contenir des NaN\n",
    "a.normal_()\n",
    "print(a)\n",
    "print(torch.mean(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> **REMARQUE** Dans l'exemple précédent, la méthode `normal_()` se termine par un underscore. Cela signifie que cette méthode fait une mutation du `Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "b = torch.Tensor(10,1).fill_(1)\n",
    "print(b)\n",
    "print(a.matmul(b))\n",
    "print(torch.matmul(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On peut facilement transférer un `Tensor` sur GPU. Les opérations sur ces `Tensor` seront exécutées sur GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a_gpu = a.cuda()\n",
    "b_gpu = b.cuda()\n",
    "print(a_gpu.matmul(b_gpu))\n",
    "print(a_gpu.matmul(b_gpu).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TODO corrigez cette opération pour multiplier `a` avec `c_gpu` sur cpu\n",
    "c_gpu = a_gpu.matmul(b_gpu)\n",
    "print(a.matmul(c_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PyTorch fournit également l'API Variable dans le package `torch.autograd`. Une Variable peut être initialisée à partir d'un `Tensor`. Les API de `torch.autograd.Variable` et `torch.Tensor` sont presque identiques. Par contre, on ne peut pas faire d'opérations entre des Tensors et des Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "a_var = Variable(a)\n",
    "b_var = Variable(b)\n",
    "print(b, b_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(a_var.matmul(b_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(a_var.matmul(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "En règle générale, les opérations in-place, c'est-à-dire les opérations qui font une mutation directe d'un Tensor (et qui se termine par un underscore), ne sont pas disponibles dans l'API de Variable. Pour les autres opérations (par exemple matmul), il n'y a pas de différence entre les API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a.uniform_()\n",
    "a_var.uniform_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pourquoi deux API? Variable englobe un Tensor et contient des champs supplémentaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(type(a_var))\n",
    "print(type(a_var.data))\n",
    "print(a_var.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Variable sert à construire les graphe de calcul et faire la backpropagation du gradient. C'est le sujet de la section suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Gradient et Backpropagation\n",
    "\n",
    "Variable provient du package `torch.autograd`. Comme le nom du package l'indique, il est possible d'automatiquement calculer la dérivée de fonctions calculée à partir d'opérations sur les variables. On indique les Variables qu'on veut dériver avec `requires_grad=True` (par défaut à False). Dans l'exemple suivant, lors du calcul de `w` (propagation avant), PyTorch construit dynamiquement un graphe de calcul indiquant les liens de dépendance entre les variables et les opérations, ce qui permet la backpropagation.\n",
    "\n",
    "> **NOTE** Contrairement à des librairies comme Tensorflow où le graphe de calcul est statique, PyTorch recrée dynamique le graphe de calcul à chaque itération. Cela permet de modifier la structure du graphe dynamiquement avec du code Python. Par contre, cela rend la visualisation du graphe plus difficile. \n",
    "\n",
    "- Autograd\n",
    "- Appeler backprop deux fois (qu'est-ce qui arrive)?\n",
    "- Volatile\n",
    "- requires_gradient true et false pour les variables à entraîner vs. les inputs et les variables freezées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(3, 3).uniform_(-1, 1))\n",
    "y = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "z = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "w = torch.matmul(x, y) + x + y + z\n",
    "print(x, y, z, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(w.grad_fn)\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "print(w.grad)\n",
    "\n",
    "make_vizualization_autograd(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w_grad = torch.ones(w.size())\n",
    "w.backward(w_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Questions\n",
    "- Exécutez deux fois la cellule qui appelle la fonction .backward(). Qu'arrive-t-il? Pourquoi?\n",
    "- Quelles variables auraient requires_grad=False dans le contexte d'entraînement de réseaux de neurones?\n",
    "- Dans l'exemple précédent, pourquoi `w` n'a-t-il pas de gradient?\n",
    "\n",
    "##### TODO exercice\n",
    "Faites la mise-à-jour des valeurs de y et z et soustrayant $1 \\times 10^{-3}$ fois leur gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Volatile\n",
    "Un autre *flag* permet d'indiquer à PyTorch de ne pas calculer le gradient d'une variable. Il s'agit du flag **`volatile=True`**. Par contre, son comportement est différent de `requires_grad=False`. Si une variable est volatile, toutes les variables en sortie des opérations dépendant de la variables volatile seront aussi volatile. Dans le premier exemple, w requière un calcul de gradient car au moins une variable (ici y et z) plus haut dans l'arbre de calcul de la backprop le requière également. Au contraire, dans le deuxième exemple, w ne requière pas de calcul de gradient car au moins une variable (ici x) dans plus haut dans l'arbre de calcul de la backprop est volatile. Cela fait que w est volatile également."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(3, 3).uniform_(-1, 1))\n",
    "y = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "z = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "w = torch.matmul(x, y) + x + y + z\n",
    "\n",
    "print(w.requires_grad, w.volatile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(3, 3).uniform_(-1, 1), volatile=True)\n",
    "y = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "z = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "w = torch.matmul(x, y) + x + y + z\n",
    "\n",
    "print(w.requires_grad, w.volatile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Question\n",
    "- Dans quel contexte voudrait-on ne calculer aucun gradient d'un graphe de calcul?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fonction d'activation\n",
    "- Avantage de la ReLU sur sigmoid vs. tanh\n",
    "- Exemple sur le vanishing gradient\n",
    "- Réduction d'un réseau à plusieur couches sans non-linéarité à un réseau à une seule couche.\n",
    "\n",
    "#### Visualisation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from deeplib.data import SpiralDataset, train_valid_loaders\n",
    "\n",
    "dataset = SpiralDataset()\n",
    "points, labels = dataset.to_numpy()\n",
    "plt.scatter(points[labels==1,0], points[labels==1,1])\n",
    "plt.scatter(points[labels==0,0], points[labels==0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TIPS** Écrire ici comment fonctionnent `__iter__`, `__next__`, `iter()` et `next()` en python. itertools.islice en python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader = train_valid_loaders(dataset, 8)\n",
    "\n",
    "for i, (data, label) in enumerate(train_loader):\n",
    "    print(data, label)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "for data in itertools.islice(iter(train_loader), 10):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RandomModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345) # Both Tanh model and ReLU model will have the same random weights\n",
    "        \n",
    "        self.layers = []\n",
    "        for i in range(n_layers):\n",
    "            layer = nn.Linear(7,7)\n",
    "            layer.weight.data.normal_(0.0, math.sqrt(2 / 7))\n",
    "            layer.bias.data.fill_(0)\n",
    "            self.layers.append(layer)\n",
    "            self.add_module('layer-%d' % i, layer)\n",
    "        self.output_layer = nn.Linear(7,2)\n",
    "\n",
    "        self.nonzero_grad_stats = None\n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        raise NotImplementedError('Defined in children classes')\n",
    "       \n",
    "    \n",
    "    def _forward_output_layer(self, x):\n",
    "        out = self.output_layer.forward(x)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def print_weights_grads(self):\n",
    "        self.nonzero_grad_stats = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(\"-----\\nLayer %d\" % i)\n",
    "            print(\"Weight:\\n%sWeight gradient:\\n%s\\n\" % (str(layer.weight.data), \n",
    "                                                         str(layer.weight.grad)))\n",
    "            if layer.weight.grad is not None:\n",
    "                nonzero_grad_indices = torch.nonzero(layer.weight.grad.data)\n",
    "                nonzero_grad = [layer.weight.grad.data[i,j] for (i,j) in nonzero_grad_indices]\n",
    "                nonzero_grad_mean = np.mean(np.abs(nonzero_grad))\n",
    "                self.nonzero_grad_stats.append((len(nonzero_grad), nonzero_grad_mean))\n",
    "                print(\"Number of nonzero gradient: %f\" % len(nonzero_grad))\n",
    "                print(\"Nonzero grad mean: %f\" % nonzero_grad_mean)\n",
    "        \n",
    "\n",
    "        \n",
    "class RandomReluModel(RandomModel):\n",
    "    \n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__(n_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "            out = F.relu(out)\n",
    "        return self._forward_output_layer(out)\n",
    "        \n",
    "        \n",
    "        \n",
    "class RandomTanhModel(RandomModel):\n",
    "    \n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__(n_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "            out = F.tanh(out)\n",
    "        return self._forward_output_layer(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "relu_model = RandomReluModel(10)\n",
    "tanh_model = RandomTanhModel(10)\n",
    "relu_model.print_weights_grads()\n",
    "tanh_model.print_weights_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_in, data_out = next(iter(train_loader))\n",
    "relu_output = relu_model.forward(Variable(data_in))\n",
    "tanh_output = tanh_model.forward(Variable(data_in))\n",
    "print(data_in)\n",
    "print(\"ReLU model ouput:\\n\", relu_output)\n",
    "print(\"tanh model ouput:\\n\", tanh_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### TODO Exercice\n",
    "Vérifiez que le réseau retourne bel et bien des probabilités. Identifiez la ligne de code qui transforme des nombres arbitraires en probabilité. Indice: il y a une erreur volontaire dans le code.\n",
    "\n",
    "#### Analyse du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.NLLLoss()\n",
    "relu_loss = loss(relu_output, Variable(data_out))\n",
    "tanh_loss = loss(tanh_output, Variable(data_out))\n",
    "print(relu_loss, tanh_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "relu_loss.backward()\n",
    "tanh_loss.backward()\n",
    "relu_model.print_weights_grads()\n",
    "tanh_model.print_weights_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(relu_model.nonzero_grad_stats)), [x[0] for x in relu_model.nonzero_grad_stats])\n",
    "plt.plot(np.arange(len(tanh_model.nonzero_grad_stats)), [x[0] for x in tanh_model.nonzero_grad_stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "axs[0].plot(np.arange(len(relu_model.nonzero_grad_stats)), [x[1] / x[0] for x in relu_model.nonzero_grad_stats])\n",
    "axs[0].plot(np.arange(len(tanh_model.nonzero_grad_stats)), [x[1] / x[0] for x in tanh_model.nonzero_grad_stats])\n",
    "axs[1].plot(np.arange(4), [x[1] / x[0] for x in relu_model.nonzero_grad_stats[:4]])\n",
    "axs[1].plot(np.arange(4), [x[1] / x[0] for x in tanh_model.nonzero_grad_stats[:4]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "heatmap = np.zeros((7,7))\n",
    "for data in train_loader:\n",
    "    data_in = Variable(data[0])\n",
    "    relu_model.forward(data_in)\n",
    "    nonzero_grad_indices = torch.nonzero(relu_model.layers[6].weight.grad.data)\n",
    "    for (i, j) in nonzero_grad_indices:\n",
    "        heatmap[i,j] += 1\n",
    "print(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "n_epoch = 1000\n",
    "relu_losses = []\n",
    "tanh_losses = []\n",
    "relu_optimizer = SGD(relu_model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "tanh_optimizer = SGD(tanh_model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"================\\nEpoch %d done.\" % epoch)\n",
    "    relu_epoch_losses = []\n",
    "    tanh_epoch_losses = []\n",
    "    for data_in, data_out in map(lambda data: (Variable(data[0]), Variable(data[1])),\n",
    "                                 train_loader):\n",
    "        relu_optimizer.zero_grad()\n",
    "        tanh_optimizer.zero_grad()\n",
    "        \n",
    "        relu_loss = loss(relu_model(data_in), data_out)\n",
    "        tanh_loss = loss(tanh_model(data_in), data_out)\n",
    "        relu_epoch_losses.append(float(relu_loss))\n",
    "        tanh_epoch_losses.append(float(tanh_loss))\n",
    "        \n",
    "        relu_loss.backward()\n",
    "        tanh_loss.backward()\n",
    "        relu_optimizer.step()\n",
    "        tanh_optimizer.step()\n",
    "    relu_losses.append(np.mean(np.asarray(relu_epoch_losses)))\n",
    "    tanh_losses.append(np.mean(np.asarray(tanh_epoch_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(relu_losses)), np.asarray(relu_losses))\n",
    "plt.plot(np.arange(len(tanh_losses)), np.asarray(tanh_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Questions\n",
    "- Observez la distribution du gradient lors de la backprop. Quelles différences y a-t-il entre la backprop à travers ReLU et à travers tanh?\n",
    "- Est-ce que, pour deux entrées différentes, les mêmes poids ont un gradient élevé?\n",
    "- Changez le nombre de couches du réseau. Qu'observez-vous?\n",
    "- Changez la moyenne de la gaussienne des poids lors de l'initilisation. Qu'observez-vous?\n",
    "- Identifiez un problème avec la tanh. Identifiez un problème avec la ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glo4030-7030",
   "language": "python",
   "name": "glo4030-7030"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
