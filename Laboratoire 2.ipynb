{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Laboratoire 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from deeplib.visualization import make_vizualization_autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Graphe computationnel et backprop\n",
    "Cette section a pour but de vous familiariser avec les notions de graphe computationnel et de backpropagation, plus particulièrement leur implémentation PyTorch. Dans le dernier laboratoire, vous avez vu une version haut-niveau de l'entraînement de réseaux de neurones. À l'inverse, ce laboratoire a pour but de vous donner une intuition du fonctionnement interne de PyTorch. Qui sait? Peut-être voudrai vous un jour implémenter vous même votre librairie de graphe de calcul.\n",
    "\n",
    "#### Tenseurs et Variables\n",
    "La structure de données de base dans PyTorch est le `Tensor`. Cette structure de données est comparable au `ndarray` numpy. Le package `torch.Tensor` défini des matrices multidimensionnelles et les opérations sur celles-ci. Voici quelques exemples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Création et initialisation à une normale centrée à 0 et de variance 1.\n",
    "a = torch.Tensor(10,10)\n",
    "print(a) # Initialement, le tenseur contient du 'garbage'. Il peut même contenir des NaN\n",
    "a.normal_()\n",
    "print(a)\n",
    "print(torch.mean(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> **REMARQUE** Dans l'exemple précédent, la méthode `normal_()` se termine par un underscore. Cela signifie que cette méthode fait une mutation du `Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "b = torch.Tensor(10,1).fill_(1)\n",
    "print(b)\n",
    "print(a.matmul(b))\n",
    "print(torch.matmul(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On peut facilement transférer un `Tensor` sur GPU. Les opérations sur ces `Tensor` seront exécutées sur GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a_gpu = a.cuda()\n",
    "b_gpu = b.cuda()\n",
    "print(a_gpu.matmul(b_gpu))\n",
    "print(a_gpu.matmul(b_gpu).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TODO corrigez cette opération pour multiplier `a` avec `c_gpu` sur cpu\n",
    "c_gpu = a_gpu.matmul(b_gpu)\n",
    "print(a.matmul(c_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PyTorch fournit également l'API Variable dans le package `torch.autograd`. Une Variable peut être initialisée à partir d'un `Tensor`. Les API de `torch.autograd.Variable` et `torch.Tensor` sont presque identiques. Par contre, on ne peut pas faire d'opérations entre des Tensors et des Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "a_var = Variable(a)\n",
    "b_var = Variable(b)\n",
    "print(b, b_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(a_var.matmul(b_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(a_var.matmul(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "En règle générale, les opérations in-place, c'est-à-dire les opérations qui font une mutation directe d'un Tensor (et qui se terminent par un underscore), ne sont pas disponibles dans l'API de Variable. Pour les autres opérations (par exemple matmul), il n'y a pas de différence entre les API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a.uniform_()\n",
    "a_var.uniform_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pourquoi deux API? Variable englobe un Tensor et contient des attributs supplémentaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(type(a_var))\n",
    "print(type(a_var.data))\n",
    "print(a_var.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Variable sert à construire les graphe de calcul et faire la backpropagation du gradient. C'est le sujet de la section suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Gradient et Backpropagation\n",
    "\n",
    "Variable provient du package `torch.autograd`. Comme le nom du package l'indique, il est possible d'automatiquement calculer la dérivée de fonctions calculées à partir d'opérations sur les variables. On indique les Variables qu'on veut dériver avec `requires_grad=True` (par défaut à False). Dans l'exemple suivant, lors du calcul de `w` (propagation avant), PyTorch construit dynamiquement un graphe de calcul indiquant les liens de dépendance entre les variables et les opérations, ce qui permet la backpropagation.\n",
    "\n",
    "> **NOTE** Contrairement à des librairies comme Tensorflow où le graphe de calcul est statique, PyTorch recrée dynamique le graphe de calcul à chaque itération. Cela permet de modifier la structure du graphe dynamiquement avec du code Python. Par contre, cela rend la visualisation du graphe plus difficile. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(3, 3).uniform_(-1, 1))\n",
    "y = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "z = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "f = torch.matmul(x, y) + x + y + z\n",
    "print(x, y, z, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(f.grad_fn)\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "print(f.grad)\n",
    "\n",
    "make_vizualization_autograd(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f_grad = torch.ones(f.size())\n",
    "f.backward(f_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "print(f.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Questions\n",
    "- Exécutez deux fois la cellule qui appelle la fonction .backward(). Qu'arrive-t-il? Pourquoi?\n",
    "- Quelles variables auraient requires_grad=False dans le contexte d'entraînement de réseaux de neurones?\n",
    "- Dans l'exemple précédent, pourquoi `f` n'a-t-il pas de gradient?\n",
    "\n",
    "##### TODO exercice\n",
    "Faites la mise-à-jour des valeurs de y et z et soustrayant $1 \\times 10^{-3}$ fois leur gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Volatile\n",
    "Un autre *flag* permet d'indiquer à PyTorch de ne pas calculer le gradient d'une variable. Il s'agit du flag **`volatile=True`**. Par contre, son comportement est différent de `requires_grad=False`. Si une variable est volatile, toutes les variables en sortie des opérations dépendant de la variable volatile seront aussi volatiles. Dans le premier exemple, f requière un calcul de gradient car au moins une variable (ici y et z) plus haut dans l'arbre de calcul de la backprop le requière également. Au contraire, dans le deuxième exemple, f ne requière pas de calcul de gradient car au moins une variable (ici x) dans plus haut dans l'arbre de calcul de la backprop est volatile. Cela fait que f est volatile également.\n",
    "\n",
    "> **NOTE** Ne pas confondre `volatile=True` avec le symbole réservé en C `volatile`. Les deux représentent des concepts complètement différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(3, 3).uniform_(-1, 1))\n",
    "y = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "z = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "f = torch.matmul(x, y) + x + y + z\n",
    "\n",
    "print(f.requires_grad, f.volatile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor(3, 3).uniform_(-1, 1), volatile=True)\n",
    "y = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "z = Variable(torch.Tensor(3, 3).uniform_(-1, 1), requires_grad=True)\n",
    "f = torch.matmul(x, y) + x + y + z\n",
    "\n",
    "print(f.requires_grad, f.volatile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Question\n",
    "- Dans quel contexte voudrait-on ne calculer aucun gradient d'un graphe de calcul?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fonction d'activation\n",
    "\n",
    "La section suivante a pour but d'explorer les différences entre les fonctions d'activation ReLU et Tanh.\n",
    "\n",
    "#### Question préalable\n",
    "- À quoi sert la fonction d'activation? Sans elle, que devient un réseau multi-couches?\n",
    "\n",
    "#### Visualisation du dataset\n",
    "Pour cette partie, nous utiliserons le dataset des spirales. Vous pouvez voir le code qui a servi à générer le dataset dans la librairie https://github.com/ulaval-damas/glo4030-labs/blob/master/deeplib/datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from deeplib.datasets import SpiralDataset, train_valid_loaders\n",
    "\n",
    "dataset = SpiralDataset()\n",
    "points, labels = dataset.to_numpy()\n",
    "print(points.shape, labels.shape)\n",
    "plt.scatter(points[labels==1,0], points[labels==1,1])\n",
    "plt.scatter(points[labels==0,0], points[labels==0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Dans la cellule suivante,  `train_valid_loaders()` retourne des [DataLoader](http://pytorch.org/docs/0.3.0/data.html#torch.utils.data.DataLoader) pytorch. Il est possible d'itérer sur les données d'un `DataLoader` comme on le ferait sur un liste python.\n",
    "\n",
    "> **PYTHON DEEP DIVE** Pourquoi peut-on itérer facilement sur le `DataLoader`? Parce que la classe implémente la méthode `__iter__`. Ainsi, quand on fait\n",
    "```\n",
    "for x in data_loader:\n",
    "```\n",
    "la fonction `__iter__` est appelée, ce qui crée un itérateur sur le `DataLoader`. Cet itérateur peut charger les données sur demande puis que la méthode `__next__` est appelée à chaque itération. Il est possible de créer explicitement cet itérateur en appelant la fonction built-in `iter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader = train_valid_loaders(dataset, 8)\n",
    "\n",
    "for i, (data, label) in enumerate(train_loader):\n",
    "    print(data, label)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Comme pour tout itérateur python, on peut utiliser `itertools`. Par exemple, ici, on boucle sur les 10 premiers éléments de l'itérateur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "for data in itertools.islice(iter(train_loader), 10):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Création de modèles\n",
    "\n",
    "Ici, on crée des classes qui héritent de `torch.nn.Module`. C'est la classe de base de tout réseau dans pytorch. `Module` comporte par exemple la méthode `named_parameters()` qui permet d'obtenir toutes les variables entraînables du `Module` ainsi que leur nom. Voici un lien vers la documentation complète:\n",
    "http://pytorch.org/docs/0.3.0/nn.html#torch.nn.Module.\n",
    "\n",
    "##### Exercice\n",
    "Écrivez la fonction forward de TanhModel et ReluModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RandomModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345) # Both Tanh model and ReLU model will have the same random weights\n",
    "        \n",
    "        self.layers = []\n",
    "        for i in range(n_layers):\n",
    "            layer = nn.Linear(7,7)\n",
    "            layer.weight.data.normal_(0.0, math.sqrt(2 / 7))\n",
    "            layer.bias.data.fill_(0)\n",
    "            self.layers.append(layer)\n",
    "            self.add_module('layer-%d' % i, layer)\n",
    "        self.output_layer = nn.Linear(7,2)\n",
    "\n",
    "        self.nonzero_grad_stats = None\n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        raise NotImplementedError('Defined in children classes')\n",
    "       \n",
    "    \n",
    "    def _forward_output_layer(self, x):\n",
    "        out = self.output_layer.forward(x)\n",
    "        out = F.log_softmax(out, dim=0)\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def print_weights_grads(self):\n",
    "        self.nonzero_grad_stats = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(\"-----\\nLayer %d\" % i)\n",
    "            print(\"Weight:\\n%sWeight gradient:\\n%s\\n\" % (str(layer.weight.data), \n",
    "                                                         str(layer.weight.grad)))\n",
    "            if layer.weight.grad is not None:\n",
    "                nonzero_grad_indices = torch.nonzero(layer.weight.grad.data)\n",
    "                nonzero_grad = [layer.weight.grad.data[i,j] for (i,j) in nonzero_grad_indices]\n",
    "                nonzero_grad_mean = np.mean(np.abs(nonzero_grad))\n",
    "                self.nonzero_grad_stats.append((len(nonzero_grad), nonzero_grad_mean))\n",
    "                print(\"Number of nonzero gradient: %f\" % len(nonzero_grad))\n",
    "                print(\"Nonzero grad mean: %f\" % nonzero_grad_mean)\n",
    "        \n",
    "\n",
    "        \n",
    "class RandomReluModel(RandomModel):\n",
    "    \n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__(n_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "class RandomTanhModel(RandomModel):\n",
    "    \n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__(n_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "relu_model = RandomReluModel(10)\n",
    "tanh_model = RandomTanhModel(10)\n",
    "relu_model.print_weights_grads()\n",
    "tanh_model.print_weights_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_in, data_out = next(iter(train_loader))\n",
    "relu_output = relu_model.forward(Variable(data_in))\n",
    "tanh_output = tanh_model.forward(Variable(data_in))\n",
    "print(data_in)\n",
    "print(\"ReLU model ouput:\\n\", relu_output)\n",
    "print(\"tanh model ouput:\\n\", tanh_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### TODO Exercice\n",
    "Vérifiez que le réseau retourne bel et bien des probabilités. Identifiez la ligne de code qui transforme des nombres arbitraires en probabilité. Indice: il y a une erreur volontaire dans le code que vous devez corriger.\n",
    "\n",
    "#### Analyse du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.NLLLoss()\n",
    "relu_loss = loss(relu_output, Variable(data_out))\n",
    "tanh_loss = loss(tanh_output, Variable(data_out))\n",
    "print(relu_loss, tanh_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "relu_loss.backward()\n",
    "tanh_loss.backward()\n",
    "relu_model.print_weights_grads()\n",
    "tanh_model.print_weights_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Le graphique suivant représente la quantité de poids qui ont un gradient nul lors de la backprop en fonction du numéro de la couche. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(relu_model.nonzero_grad_stats)), [x[0] for x in relu_model.nonzero_grad_stats], label='ReLU')\n",
    "plt.plot(np.arange(len(tanh_model.nonzero_grad_stats)), [x[0] for x in tanh_model.nonzero_grad_stats], label='Tanh')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Le graphique suivant représente le gradient moyen (sans tenir compte des gradients nuls) en fonction du numéro de couche. La partie du bas est un zoom sur les premières couches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "axs[0].plot(np.arange(len(relu_model.nonzero_grad_stats)), [x[1] / x[0] for x in relu_model.nonzero_grad_stats],label='ReLU')\n",
    "axs[0].plot(np.arange(len(tanh_model.nonzero_grad_stats)), [x[1] / x[0] for x in tanh_model.nonzero_grad_stats],label='Tanh')\n",
    "axs[0].legend()\n",
    "axs[1].plot(np.arange(4), [x[1] / x[0] for x in relu_model.nonzero_grad_stats[:4]])\n",
    "axs[1].plot(np.arange(4), [x[1] / x[0] for x in tanh_model.nonzero_grad_stats[:4]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "La cellule suivante compte le nombre de fois que chaque poids a un gradient non-nul en passant plusieurs points dans le réseaux et en incrémentant le compteur à chaque fois que cela arrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layer_index = 1\n",
    "heatmap = np.zeros((7,7))\n",
    "n_batch = 0\n",
    "for data in train_loader:\n",
    "    n_batch += 1\n",
    "    data_in = Variable(data[0])\n",
    "    relu_model.forward(data_in)\n",
    "    nonzero_grad_indices = torch.nonzero(relu_model.layers[layer_index].weight.grad.data)\n",
    "    for (i, j) in nonzero_grad_indices:\n",
    "        heatmap[i,j] += 1\n",
    "print(n_batch)\n",
    "print(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "n_epoch = 1000\n",
    "relu_losses = []\n",
    "tanh_losses = []\n",
    "relu_optimizer = SGD(relu_model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "tanh_optimizer = SGD(tanh_model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"================\\nEpoch %d done.\" % epoch)\n",
    "    relu_epoch_losses = []\n",
    "    tanh_epoch_losses = []\n",
    "    for data_in, data_out in map(lambda data: (Variable(data[0]), Variable(data[1])),\n",
    "                                 train_loader):\n",
    "        relu_optimizer.zero_grad()\n",
    "        tanh_optimizer.zero_grad()\n",
    "        \n",
    "        relu_loss = loss(relu_model(data_in), data_out)\n",
    "        tanh_loss = loss(tanh_model(data_in), data_out)\n",
    "        relu_epoch_losses.append(float(relu_loss))\n",
    "        tanh_epoch_losses.append(float(tanh_loss))\n",
    "        \n",
    "        relu_loss.backward()\n",
    "        tanh_loss.backward()\n",
    "        relu_optimizer.step()\n",
    "        tanh_optimizer.step()\n",
    "    relu_losses.append(np.mean(np.asarray(relu_epoch_losses)))\n",
    "    tanh_losses.append(np.mean(np.asarray(tanh_epoch_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(relu_losses)), np.asarray(relu_losses),label='ReLU')\n",
    "plt.plot(np.arange(len(tanh_losses)), np.asarray(tanh_losses),label='Tanh')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Questions\n",
    "- Observez la distribution du gradient lors de la backprop. Quelles différences y a-t-il entre la backprop à travers ReLU et à travers tanh?\n",
    "- Est-ce que, pour deux entrées différentes, les mêmes poids ont un gradient élevé?\n",
    "- Changez le nombre de couches du réseau. Qu'observez-vous?\n",
    "- Changez la moyenne de la gaussienne des poids lors de l'initilisation. Qu'observez-vous?\n",
    "- Identifiez un problème avec la tanh. Identifiez un problème avec la ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glo4030-7030",
   "language": "python",
   "name": "glo4030-7030"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
