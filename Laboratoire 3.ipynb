{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 3: Optimisation\n",
    "\n",
    "## 1. Fonctions d'optimisation\n",
    "\n",
    "Dans cette section, vous testerez différentes fonctions d'optimisation et observerez leurs effets sur l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from deeplib.history import History\n",
    "from deeplib.datasets import train_valid_loaders, load_cifar10, load_mnist\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from deeplib.net import CifarNet, MnistNet\n",
    "\n",
    "from deeplib.training import train, validate_ranking, test\n",
    "from deeplib.visualization import show_worst, show_random, show_best\n",
    "\n",
    "cifar_train, cifar_test = load_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple d'entraînement avec SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.01\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CifarNet()\n",
    "model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "history_sgd = train(model, optimizer, cifar_train, n_epoch, batch_size)\n",
    "history_sgd.display_accuracy()\n",
    "history_sgd.display_loss()\n",
    "print('Précision en test: {:.2f}'.format(test(model, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "Comparez trois différentes stratégies d'optimisation:\n",
    "1. [SGD](http://pytorch.org/docs/master/optim.html#torch.optim.SGD)\n",
    "2. SGD + Momentum accéléré de Nesterov\n",
    "3. [Adam](http://pytorch.org/docs/master/optim.html#torch.optim.Adam) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez cette cellule pour entraîner avec SGD + Momentum accéléré de Nesterov. Utilisez un momentum de 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CifarNet()\n",
    "model.cuda()\n",
    "# optimizer = \n",
    "history_SGDMN = train(model, optimizer, cifar_train, n_epoch, batch_size)\n",
    "history_SGDMN.display_accuracy()\n",
    "history_SGDMN.display_loss()\n",
    "print('Précision en test: {:.2f}'.format(test(model, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez cette cellule pour entraîner avec Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CifarNet()\n",
    "model.cuda()\n",
    "# optimizer = \n",
    "history_adam = train(model, optimizer, cifar_train, n_epoch, batch_size)\n",
    "history_adam.display_accuracy()\n",
    "history_adam.display_loss()\n",
    "print('Précision en test: {:.2f}'.format(test(model, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle méthode semble être la meilleure dans ce cas-ci?\n",
    "Remarquez-vous une différence d'overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Horaire d'entraînement\n",
    "\n",
    "Une pratique courante utilisé en deep learning est de faire diminuer le learning rate pendant l'entraînement.\n",
    "\n",
    "Pour ce faire PyTorch fourni plusieurs fonctions (ExponentialLR, LambdaLR, MultiStepLR, etc.)\n",
    "\n",
    "Voici un exemple avec ExponentialLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CifarNet()\n",
    "model.cuda()\n",
    "\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "n_epoch = 10\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "gamma = 0.99\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "\n",
    "history = train(model, optimizer, cifar_train, n_epoch, batch_size, scheduler=scheduler, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history.display_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "Utilisez [MultiStepLR](http://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.MultiStepLR) pour modifier le learning rate un epoch précis. \n",
    "\n",
    "1. Commencez avec un learning rate trop élevé pour que le réseau puisse apprendre quelque chose.\n",
    "2. Diminuez le progressivement jusqu'à ce que le réseau apprenne.\n",
    "3. Trouvez le moment où la validation semble avoir atteint un plateau.\n",
    "4. Diminuez le learning par 2 à ce moment et réentraîner le réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = CifarNet()\n",
    "model.cuda()\n",
    "\n",
    "epoch_list = []\n",
    "\n",
    "batch_size = 128\n",
    "lr = 10\n",
    "n_epoch = 20\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=epoch_list, gamma=0.5)\n",
    "\n",
    "history = train(model, optimizer, cifar_train, n_epoch, batch_size, scheduler=scheduler, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history.display_lr()\n",
    "history.display_loss()\n",
    "history.display_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyez-vous une différence en diminuant le learning rate par 2 après x epochs?<br>\n",
    "Pourquoi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Normalization\n",
    "\n",
    "Voici l'architecture du réseau de neurones convolutionnels que vous avez utilisé jusqu'à présent pour faire de la classification sur Cifar10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CifarNetBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CifarNetBatchNorm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 50, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(50, 150, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(150 * 8 * 8, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def num_flat_features(x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "Modifier l'architecture du réseau en ajoutant de la batch norm entre les couches de convolutions et entraîner le nouveau réseau.\n",
    "\n",
    "Comparer l'entraînement du réseau avec et sans la batch norm (Section 1 avec SGD).<br>\n",
    "Que remarquez-vous?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CifarNetBatchNorm()\n",
    "model.cuda()\n",
    "\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "n_epoch = 10\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "history = train(model, optimizer, cifar_train, n_epoch, batch_size, use_gpu=True)\n",
    "history.display_accuracy()\n",
    "history.display_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effet de la batch norm sur le learning rate\n",
    "\n",
    "Commençons par entraîner un réseau avec un haut learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "batch_size = 1024\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CifarNet()\n",
    "model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "history = train(model, optimizer, cifar_train, n_epoch, batch_size, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons maintenant d'entraîner le réseau utilisant la batchnorm avec les mêmes hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CifarNetBatchNorm()\n",
    "model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "history = train(model, optimizer, cifar_train, n_epoch, batch_size, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que pouvez-vous conclure sur l'effet de la batch norm sur le learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse\n",
    "\n",
    "Après l'entraînement, il est important d'analyser les résultats obtenus.\n",
    "Commençons par tester le réseau en utilisant la fonction `validate_ranking`.\n",
    "Cette fonction sépare les résultats bien classés des erreurs et retourne pour chaque image, un score (qu'on peut voir comme une probabilité), la vraie classe et la classe prédite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cifar_test.transform = ToTensor()\n",
    "loader, _ = train_valid_loaders(cifar_test, batch_size, train_split=1)\n",
    "good, errors = validate_ranking(model, loader, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, regardons quelques exemples d'images bien classées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_random(good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et quelques exemples mal classées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_random(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est aussi possible de regarder les exemples où le réseau est le plus confiant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_best(good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou l'inverse, ceux qui ont obtenus les moins bons scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_worst(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, il peut être intéressant de regarder les exemples les plus difficiles.\n",
    "Soit ceux qui ont été bien classé, mais qui ont eu un mauvais score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_worst(good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou ceux qui été mal classé, mais qui ont quand même réussi à obtenir un bon score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_best(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En observant les résultats obtenus, que pouvez-vous dire sur les performances du réseau? <br>\n",
    "Quelle classe semble être facile? Pourquoi? <br>\n",
    "Quelle classe semble être difficile? Pourquoi? <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
